{
  "markdown": "nature machine intelligence <!-- marginalia, from page 0 (l=0.062,t=0.048,r=0.420,b=0.077), with ID b44749b6-92c2-4e9f-bc79-aabb1a3de6ba -->\n\nSummary : This is a logo representing the concept of open access, commonly used in academic publishing to indicate that content is freely available to the public.\n\nlogo:  \nDesign Elements :  \n  ‚Ä¢ Orange open padlock symbol, with a circular body and a semicircular shackle open at the top left.  \n  ‚Ä¢ The padlock body contains a smaller filled orange circle at its center.  \n  ‚Ä¢ No text, company name, or tagline is present.  \n  ‚Ä¢ The logo is set against a white background with a thin blue line at the very top edge.\n\nDimensions & Placement :  \n  ‚Ä¢ The logo is vertically oriented, with the shackle at the top and the circular body below.  \n  ‚Ä¢ The blue line is horizontal and positioned at the very top of the image.\n\nAnalysis :  \n  ‚Ä¢ The open padlock visually communicates unrestricted access, aligning with the principles of open access publishing.  \n  ‚Ä¢ The use of orange is typical for open access branding, making the symbol easily recognizable in academic contexts. <!-- figure, from page 0 (l=0.904,t=0.049,r=0.949,b=0.086), with ID 76ebb208-f221-4c7f-a90e-536f844f56d2 -->\n\nArticle <!-- text, from page 0 (l=0.060,t=0.114,r=0.128,b=0.133), with ID f06c699d-02e6-4214-afc3-ee8eeb73313d -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.649,t=0.115,r=0.949,b=0.135), with ID 971555ca-6b04-46ad-abb7-a147c43040df -->\n\nEmbodied large language models enable\nrobots to complete complex tasks in\nunpredictable environments <!-- text, from page 0 (l=0.060,t=0.140,r=0.863,b=0.251), with ID 33f62124-a73a-413e-aedd-ce6e8bb63bb9 -->\n\nReceived: 22 June 2024\n\nAccepted: 31 January 2025\n\nPublished online: 19 March 2025\n\nüü† Check for updates <!-- text, from page 0 (l=0.061,t=0.300,r=0.349,b=0.402), with ID 22629fba-48e0-4a43-9b60-8e8d5043b5f6 -->\n\nRuaridh Mon-Williams ![inline figure: circled \"i\" icon](ORCID)¬π,¬≤,¬≥ ‚úâ, Gen Li ![inline figure: circled \"i\" icon](ORCID)¬π ‚úâ, Ran Long ![inline figure: circled \"i\" icon](ORCID)¬π, Wenqian Du ![inline figure: circled \"i\" icon](ORCID)¬π,‚Å¥ & Christopher G. Lucas¬π <!-- text, from page 0 (l=0.358,t=0.299,r=0.919,b=0.339), with ID 157c6f2a-6e4e-43f5-81cc-e7758d7d4da0 -->\n\nCompleting complex tasks in unpredictable settings challenges robotic\nsystems, requiring a step change in machine intelligence. Sensorimotor\nabilities are considered integral to human intelligence. Thus, biologically\ninspired machine intelligence might usefully combine artificial intelligence\nwith robotic sensorimotor capabilities. Here we report an embodied\nlarge-language-model-enabled robot (ELLMER) framework, utilizing GPT-4\nand a retrieval-augmented generation infrastructure, to enable robots to\ncomplete long-horizon tasks in unpredictable settings. The method extracts\ncontextually relevant examples from a knowledge base, producing action\nplans that incorporate force and visual feedback and enabling adaptation\nto changing conditions. We tested ELLMER on a robot tasked with coffee\nmaking and plate decoration; these tasks consist of a sequence of sub-tasks\nfrom drawer opening to pouring, each benefiting from distinct feedback\ntypes and methods. We show that the ELLMER framework allows the robot\nto complete the tasks. This demonstration marks progress towards scalable,\nefficient and ‚Äòintelligent robots‚Äô able to complete complex tasks in uncertain\nenvironments. <!-- text, from page 0 (l=0.360,t=0.362,r=0.945,b=0.664), with ID 6adcde0f-defb-4e03-ba5b-b6c3737ff06a -->\n\nIf Deep Blue (the first computer to win a chess match against a reigning\nworld champion) was truly intelligent, then should it not be able to\nmove its own pieces when playing chess? Intelligence is a multifaceted\nconstruct and, thus, difficult to define. Consequently, human intel-\nligence and its assessment is a controversial topic¬π. However, there is\na growing consensus that human intelligence is best understood as\n‚Äòembodied cognition‚Äô, where attention, language, learning, memory\nand perception are not abstract cognitive processes constrained to\nthe brain but intrinsically linked with how the body interacts with its\nsurrounding environment¬≤Àí¬≥. Indeed, there is growing evidence that\nhuman intelligence has its ontological and phylogenetic foundations\nin sensorimotor processes‚Å¥. <!-- text, from page 0 (l=0.061,t=0.683,r=0.500,b=0.847), with ID 7c0053ce-9ccd-40ff-a969-fad6940e7603 -->\n\nEmbodied cognition has theoretical implications for ‚Äòmachine\nintelligence‚Äô as it suggests that machines will be unable to demon-\nstrate some aspects of intelligence if ‚Äòcognitive‚Äô processes are not <!-- text, from page 0 (l=0.061,t=0.848,r=0.499,b=0.890), with ID 8bfbed06-74a7-4cd4-93fd-b913cd506bda -->\n\nembedded in a robotic device. This is a conjecture that is still to be\ntested, but ‚Äòintelligent robots‚Äô provide an effective way of exploring\nvarious hypotheses concerning human intelligence and advancing the\nfield of machine intelligence. More practically, effective human‚Äìrobot\ncollaboration will ultimately require robots to at least approximate\n‚Äòhuman-like‚Äô capabilities. Thus, a reasonable expectation of future\n‚Äòintelligent machines‚Äô is that they will have the potential to perform\nabstract cognitive computations as they skilfully interact with objects\nand humans within their environment‚Åµ. <!-- text, from page 0 (l=0.511,t=0.684,r=0.947,b=0.807), with ID 06024d76-3353-4a02-957f-f67eea30774e -->\n\nSo far, parallel streams of activity have advanced: (1) the sensori-\nmotor abilities of robots and (2) artificial intelligence‚Å∂. We set out to test\nthe hypothesis that these approaches can now be combined to create\na step change in the ability of robots to show human-like intelligence.\nWe further hypothesized that integrating (1) and (2) would allow robots\nto undertake the type of complex tasks that are practically useful in a <!-- text, from page 0 (l=0.512,t=0.808,r=0.946,b=0.890), with ID 3abcc5f4-f905-45e8-a6bc-cc278d5c5abd -->\n\n¬πUniversity of Edinburgh, Edinburgh, UK. ¬≤Massachusetts Institute of Technology, Boston, MA, USA. ¬≥Princeton University, Princeton, NJ, USA. ‚Å¥Alan Turing Institute, London, UK. ‚úâe-mail: ruaridh.mw@ed.ac.uk; li.gen@ed.ac.uk <!-- text, from page 0 (l=0.063,t=0.914,r=0.868,b=0.946), with ID ccf5713e-1dce-4a7b-8f26-9af81155fe81 -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.062,t=0.959,r=0.452,b=0.978), with ID 962b8177-6fd0-4f37-a1aa-2a817399dbe1 -->\n\n592 <!-- marginalia, from page 0 (l=0.914,t=0.962,r=0.950,b=0.977), with ID 0e0e0b53-8b5c-4571-b478-f3a42316e255 -->\n\nArticle <!-- marginalia, from page 0 (l=0.062,t=0.026,r=0.126,b=0.045), with ID 582562ff-abb7-4eca-823f-481c8b2aeedd -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.650,t=0.026,r=0.948,b=0.045), with ID a6698873-b0f5-4c34-8c93-304909f228b6 -->\n\nwide range of settings but currently outwith the capability of robotic\nsystems. Consider a scenario in which someone returns home feeling\nfatigued and thirsty. A robot with a sophisticated manipulation system\nis situated in the homeowner‚Äôs kitchen and is instructed to prepare\na drink. The robot decides that a reinvigorating cup of coffee needs\nto be made and handed to their carbon companion. This task‚Äî\nstraightforward for humans‚Äîencompasses a series of challenges that\ntest the limits of current robotic capabilities7‚Äì11. First, the robot must\ninterpret the information it receives and analyse its surroundings.\nNext, it may need to search the environment to locate a mug. This could\ninvolve opening drawers with unspecified opening mechanisms. Then,\nthe robot must measure and mix the precise ratio of water to coffee. This\nrequires fine-grained force control and adaptation to uncertainty if, for\nexample, the human moves the location of the mug unexpectedly9,12.\nThis scenario is a canonical example of the multifaceted nature of\ncomplex tasks in dynamic environments. Robotic systems have tradi-\ntionally struggled with these tasks because they have been unable to\nfollow high-level commands, have relied on preprogrammed responses\nand lack the flexibility to adapt seamlessly to perturbations13,14. <!-- text, from page 0 (l=0.062,t=0.057,r=0.499,b=0.317), with ID 7679520c-1039-4884-bb5b-fa271fd0207f -->\n\nReinforcement learning and imitation learning have demonstrated the effectiveness of interaction and demonstration in teaching robots to perform complex tasks. These approaches are promising¬π‚Åµ, but often struggle with adaptation to novel tasks and coping with diverse scenarios. Imitation learning also faces challenges when a robot needs to adapt to new contexts¬π‚Å∂‚Åª¬≤¬≥. Nature-inspired machine intelligence provides a potential solution to these challenges. The sophistication of human manipulation is due, in part, to the type of cognitive processes that are captured artificially by large language models (LLMs)¬≤‚Å¥‚Åª¬≤‚Å∂. LLMs offer a way to process complex instructions and adapt actions accordingly because of their advanced contextual understanding and generalization abilities¬≤‚Å∑,¬≤‚Å∏. <!-- text, from page 0 (l=0.063,t=0.318,r=0.499,b=0.481), with ID 97d3c7b6-2287-49ca-b54b-d53a6ad59a33 -->\n\nA large body of recent research has used LLMs for short-horizon tasks15,27,29. For instance, VoxPoser utilizes LLMs to perform a variety of everyday manipulation tasks15. Similarly, Robotics Transformer (RT-2) leverages large-scale web and robotic learning data, enabling robots to perform tasks beyond the training scenario with remarkable adaptability29. Hierarchical diffusion policy introduces a model structure to generate context-aware motion trajectories, which enhances task-specific motions from high-level LLM decision inputs30. However, challenges remain in effectively integrating LLMs into robotic manipulation. These challenges include complex prompting requirements, a lack of real-time interacting feedback, a dearth of LLM-driven work exploiting the use of force feedback and inefficient pipelines that hinder the seamless execution of tasks15,31. Moreover, current approaches have neglected the application of retrieval-augmented generation (RAG)32 in robotics despite RAG‚Äôs potential to continually update and refine robot knowledge with relevant and accurate examples (and increase the knowledge base without impacting performance). Robot capacity is also limited because force and visual feedback are typically not integrated in robot sensorimotor control15,33. This integration is crucial in scenarios such as pouring water into a moving cup, where vision is necessary to track the cup and force feedback is needed for pouring the desired amount of water when vision is occluded16,34,35. Thus, there is a need for an innovative approach in robot manipulation that combines the best of artificial ‚Äòcognition‚Äô with integrated ‚Äòsensorimotor‚Äô visual and force feedback capabilities to effectively execute actions in the face of uncertainty. Supplementary Section 1 provides more background on state-of-the-art approaches and their current limitations36‚Äì53. <!-- text, from page 0 (l=0.063,t=0.482,r=0.498,b=0.846), with ID 0523fc33-c640-47ab-ae30-4a37a543f34f -->\n\nEmbodied LLM-enabled robot (ELLMER) is a framework that\nintegrates approaches in artificial intelligence and sensorimotor\ncontrol to create a step change in robotic capabilities. Its usefulness\narises from its combined use of vision and force for sensorimotor\nfeedback control uniquely coupled with the cognitive capabilities\nafforded through an integrated LLM combined with RAG and a curated\nknowledge base. We hypothesized that ELLMER would allow a robot <!-- text, from page 0 (l=0.065,t=0.848,r=0.498,b=0.943), with ID 081062ec-e906-45da-8a98-cfbf1ae1aadb -->\n\nSummary : This figure presents a schematic overview of a hierarchical robot control system that integrates a large language model (LLM, specifically GPT-4) with a robot controller, vision system, and feedback loops for manipulation tasks. The diagram illustrates both high-level (task decomposition, code generation) and low-level (robot control, feedback) system components and their interactions.\n\nflowchart:\n# Nodes :\n  ‚Ä¢ User Query (rectangle): Represents user input in natural language.\n  ‚Ä¢ Transformer LLM (rounded rectangle): Processes user queries, integrates environment image and code database, decomposes tasks, retrieves and adapts code, and outputs Python code.\n  ‚Ä¢ Code (rectangle with code icon): Symbolizes generated Python code.\n  ‚Ä¢ Database (cylinder): Stores code examples and functions.\n  ‚Ä¢ Environment Image (camera icon): Azure Kinect depth camera providing visual input.\n  ‚Ä¢ Robot Controller (circle labeled \"A\"): Receives code and sends control signals to the robot.\n  ‚Ä¢ Robot Arm (robotic arm illustration): Executes actions based on controller commands.\n  ‚Ä¢ Force/Torque Sensor (rectangle): Provides force/torque feedback.\n  ‚Ä¢ Vision System (camera): Provides visual feedback for object recognition and pose estimation.\n  ‚Ä¢ Feedback Loops (arrows): Visual, force, and torque feedback paths to the controller.\n  ‚Ä¢ Trajectory Plot (3D axes with colored lines): Illustrates robot end-effector trajectories.\n\n# Connectors :\n  ‚Ä¢ User Query ‚Üí Transformer LLM (dashed arrow): User input is sent to the LLM.\n  ‚Ä¢ Transformer LLM ‚Üî Environment Image (dashed arrow): LLM receives image context.\n  ‚Ä¢ Transformer LLM ‚Üî Database (dashed arrow): LLM retrieves code examples.\n  ‚Ä¢ Transformer LLM ‚Üí Code (dashed arrow): LLM outputs Python code.\n  ‚Ä¢ Code ‚Üí Robot Controller (solid arrow): Code is sent to the controller.\n  ‚Ä¢ Robot Controller ‚Üí Robot Arm (solid arrow): Controller sends control signals (Œª).\n  ‚Ä¢ Robot Arm ‚Üí Vision System (solid arrow): Robot state is observed visually.\n  ‚Ä¢ Vision System ‚Üí Robot Controller (solid arrow): Visual feedback (s_visual) is sent to the controller.\n  ‚Ä¢ Robot Arm ‚Üí Force/Torque Sensor (solid arrow): Physical interaction is measured.\n  ‚Ä¢ Force/Torque Sensor ‚Üí Robot Controller (solid arrow): Force/torque feedback (s_force, s_torque) is sent to the controller.\n  ‚Ä¢ Robot Controller ‚Üí Trajectory Plot (solid arrow): Controller generates trajectories.\n  ‚Ä¢ Feedback Loops: Visual, force, and torque feedback are shown as separate labeled arrows.\n\n# Layout :\n  ‚Ä¢ The diagram is organized in two horizontal layers: the upper layer (above the dashed line) for high-level LLM-based planning and code generation, and the lower layer (below the dashed line) for low-level robot control and feedback.\n  ‚Ä¢ Arrows indicate the flow of information and control, with feedback loops closing from sensors back to the controller.\n\n# Analysis :\n  ‚Ä¢ The figure demonstrates a closed-loop, hierarchical robot control architecture where a transformer LLM decomposes user queries into executable code, leveraging both environmental context and a code database.\n  ‚Ä¢ The system integrates multimodal feedback (vision, force, torque) to enable precise manipulation, such as grasping and moving objects.\n  ‚Ä¢ The use of feedback loops (visual, force, torque) allows the robot to adapt to disturbances and update its actions in real time, as illustrated by the trajectory plot and feedback arrows.\n  ‚Ä¢ The architecture supports modularity and adaptability, with clear separation between high-level planning and low-level execution. <!-- figure, from page 0 (l=0.509,t=0.055,r=0.943,b=0.560), with ID 5f8fcb90-e89c-447e-a438-7e61c94a11f4 -->\n\nto make a cup of coffee for a human. We tested this hypothesis using\na seven-degrees-of-freedom Kinova robotic arm to execute the com-\nplex, force-intensive task in an uncertain environment, leveraging\nintegrated force and vision feedback. The overall system diagram is\npresented in Fig. 1. <!-- text, from page 0 (l=0.511,t=0.589,r=0.946,b=0.659), with ID bae2fc79-170a-4ca0-a62a-57727e010cfe -->\n\nResults\nLanguage integration for abstract reasoning\nWe found that our approach allowed the robot to respond to an abstract\nhigh-order verbal prompt (‚ÄòI‚Äôm tired, with friends due for cake soon.\nCan you make me a hot beverage, and decorate the plate with a random\nanimal of your choice‚Äô) and complete the desired task of making a hot\ndrink and drawing a shape on a plate. The video demonstrating this\ninteraction is provided in Supplementary Video 1. The robot responded\nto the prompt with the following sound output: ‚ÄòI‚Äôm setting things up to\nmake a hot beverage, and also to decorate the plate with a random ani-\nmal drawing. First, I‚Äôll find a mug then scoop coffee and pour hot water.\nAfter that I‚Äôll take an item, likely a pen, to draw a random animal on the\nplate. It sounds like a fun and cosy time with cake soon.‚Äô. In particular, the\nrobot chose to make a cup of coffee when it responded to the request\nfor a hot beverage for a tired human, and was able to decompose the\nhigher-order behaviour into a series of sub-tasks. <!-- text, from page 0 (l=0.512,t=0.670,r=0.946,b=0.888), with ID 839c9c66-23a4-4ff9-9fd6-53a47a2a8661 -->\n\nThe integration of GPT-4 (ref. 54) was found to equip the robot with the desired capacity for abstract reasoning. GPT-4 is a language model that enables a robot to process user queries and environmental data to break down tasks into actionable steps. Our system was able <!-- text, from page 0 (l=0.513,t=0.889,r=0.945,b=0.943), with ID 98d8e7ec-4e8c-4bc8-b92d-d13caedc1c1e -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.064,t=0.960,r=0.449,b=0.976), with ID ee22844d-8654-454b-a65b-59b88176b6bd -->\n\n593 <!-- marginalia, from page 0 (l=0.917,t=0.963,r=0.948,b=0.976), with ID 1d137b88-4f40-4f64-a0e6-3849a017b3b4 -->\n\nArticle <!-- marginalia, from page 0 (l=0.061,t=0.026,r=0.127,b=0.045), with ID 1f3a5a61-1665-4053-98d5-72a85cc3e14f -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.649,t=0.026,r=0.948,b=0.046), with ID ba3d5233-779c-4349-98c6-e56320ffd7c2 -->\n\nSummary : This photograph shows a robotic arm in the process of opening a cabinet door, demonstrating a manipulation task in a laboratory or testing environment.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm (labelled \"KINOVA\") equipped with a gripper, actively pulling open a white cabinet door with a red handle.\n  ‚Ä¢ The setting appears to be an indoor laboratory or workspace, with a neutral background and three small boxes (possibly tea or spice containers) placed on top of the cabinet.\n  ‚Ä¢ The perspective is a close-up, side view, focusing on the interaction between the robot and the door.\n\nTechnical Details :\n  ‚Ä¢ No visible scale bar or measurement units.\n  ‚Ä¢ The robotic arm is fitted with a soft, articulated gripper.\n  ‚Ä¢ The cabinet door is white with a red rectangular handle.\n  ‚Ä¢ No on-image UI elements or digital overlays.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic arm is positioned to the right, reaching leftward to grasp and open the door.\n  ‚Ä¢ The door is partially open, with the gripper holding the handle.\n  ‚Ä¢ The three boxes are aligned on top of the cabinet, in the background.\n\nAnalysis :\n  ‚Ä¢ The image visually documents the robot's ability to perform a common household or laboratory manipulation task (door opening), highlighting dexterity and precision in grasping and pulling actions.\n  ‚Ä¢ The presence of objects on the cabinet suggests a realistic, cluttered environment for testing robotic manipulation. <!-- figure, from page 0 (l=0.194,t=0.065,r=0.509,b=0.224), with ID 679656b0-9f35-4050-8cde-534921354150 -->\n\nSummary : This photograph shows a robotic arm performing a pick-and-place task, grasping a white cup in a kitchen-like environment.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm with a black gripper holding a white cup.\n  ‚Ä¢ The setting appears to be a kitchen or laboratory workspace, with a metallic container and a red wall-mounted fixture in the background.\n  ‚Ä¢ The perspective is from the side, focusing on the interaction between the robot and the cup.\n  ‚Ä¢ Lighting is even and neutral, with a colour palette dominated by white, grey, and black tones.\n\nTechnical Details :\n  ‚Ä¢ No scale bar or magnification is present.\n  ‚Ä¢ No visible on-image UI elements or text, except for partial text on the cup (\"IRE\").\n  ‚Ä¢ The robot arm has a white and grey casing with visible cabling.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic gripper is in the foreground, holding the cup near a flat surface.\n  ‚Ä¢ The cup is positioned close to a metallic container and a red fixture on the wall.\n  ‚Ä¢ The background includes a white cabinet and a metallic object with a knob.\n\nAnalysis :\n  ‚Ä¢ The image visually documents the robot's ability to grasp and manipulate objects, specifically demonstrating a pick-and-place operation with a cup in a realistic environment. <!-- figure, from page 0 (l=0.511,t=0.064,r=0.818,b=0.225), with ID 2552ed05-48eb-4c95-8a9e-8d20496d7266 -->\n\nSummary : This photograph shows a robotic arm performing a scooping action with a spoon, transferring a granular substance from an open jar, in a laboratory or kitchen-like environment.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm equipped with a gripper holding a metal spoon.\n  ‚Ä¢ The spoon is being used to scoop a brown granular substance (possibly coffee or soil) from an open glass jar.\n  ‚Ä¢ The setting includes a blue cylindrical container with utensils (possibly straws or stirrers) on the left, and a white tissue box on the right.\n  ‚Ä¢ The background features a white cabinet with red handles, suggesting a laboratory or organized workspace.\n  ‚Ä¢ The lighting is even and neutral, with no dramatic shadows or highlights.\n\nTechnical Details :\n  ‚Ä¢ No visible scale bar or measurement indicators.\n  ‚Ä¢ No on-image text or UI elements except for the label \"Scooping\" below the image and a panel label \"c\" in the upper left corner.\n  ‚Ä¢ The robotic arm appears to be a multi-jointed, industrial or research-grade manipulator.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic arm enters from the right, with the spoon extending toward the center of the image.\n  ‚Ä¢ The jar is in the foreground, slightly left of center, with its lid removed.\n  ‚Ä¢ The blue utensil holder is to the left of the jar, and the tissue box is to the right.\n  ‚Ä¢ The composition centers the action of scooping, with supporting objects arranged around the main activity.\n\nAnalysis :\n  ‚Ä¢ The image visually documents the robotic arm's ability to perform a fine-motor task (scooping) in a realistic, cluttered environment, highlighting dexterity and precision in object manipulation. <!-- figure, from page 0 (l=0.195,t=0.227,r=0.505,b=0.382), with ID 4d8f528d-0b97-41c9-abec-de0d23e112b0 -->\n\nSummary : This photograph shows a robotic arm performing the task of emptying a spoon into a mug, likely as part of an automated kitchen or service robot demonstration.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm holding a spoon over a white mug labeled \"inform\".\n  ‚Ä¢ The spoon appears to be in the process of emptying its contents into the mug.\n  ‚Ä¢ The setting is a kitchen or laboratory environment, with a black electric kettle, a metal French press, and a box of tea visible nearby.\n  ‚Ä¢ The perspective is from the side, focusing on the interaction between the robot and the mug.\n  ‚Ä¢ Lighting is even and neutral, with no dramatic shadows or highlights.\n  ‚Ä¢ The color palette is mostly neutral (white, black, metallic, and some green from the tea box).\n\nTechnical Details :\n  ‚Ä¢ No scale bar or magnification is present.\n  ‚Ä¢ On-image text includes \"inform\" on the mug and \"Emptying\" as a caption below the image.\n  ‚Ä¢ The robotic arm is equipped with a gripper designed to hold utensils.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic arm is positioned to the right, extending leftward toward the mug.\n  ‚Ä¢ The mug is placed on a flat surface in the foreground, with the kettle and French press behind it.\n  ‚Ä¢ The spoon is angled downward, directly above the mug's opening.\n\nAnalysis :\n  ‚Ä¢ The image visually demonstrates the robot's ability to perform a precise kitchen task‚Äîemptying a spoon into a mug‚Äîsuggesting progress in robotic manipulation and automation in domestic settings.\n  ‚Ä¢ The composition highlights the interaction between the robot and common kitchen objects, emphasizing practical application. <!-- figure, from page 0 (l=0.510,t=0.228,r=0.819,b=0.383), with ID 5d1e0e4c-c370-4771-99fa-bbd127327374 -->\n\nSummary : This photograph shows a robotic arm pouring hot water from a black kettle into a white mug held by a human hand, illustrating a collaborative human-robot interaction in a kitchen setting.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject: A robotic arm is pouring water from a black kettle into a white mug.\n  ‚Ä¢ Setting: Indoor kitchen or break room environment, with tiled wall and tea supplies (Yorkshire Tea box, other containers) visible in the background.\n  ‚Ä¢ Perspective: Side view, focused on the pouring action.\n  ‚Ä¢ Lighting: Even, indoor lighting; no harsh shadows.\n  ‚Ä¢ Colour palette: Neutral tones (white, grey, black) with some red and green accents.\n\nTechnical Details :\n  ‚Ä¢ No scale bar or magnification indicated.\n  ‚Ä¢ On-image text: The mug has a logo and the word \"inform\" printed on it.\n  ‚Ä¢ No visible UI elements or digital overlays.\n\nSpatial Relationships :\n  ‚Ä¢ Foreground: Robotic arm, kettle, mug, and human hand.\n  ‚Ä¢ Background: Tiled wall with red switches, tea boxes, and other kitchen items.\n  ‚Ä¢ The pouring action is centered, with the mug held steady by a human hand on the left and the robotic arm on the right.\n\nAnalysis :\n  ‚Ä¢ The image demonstrates a precise, collaborative task where a robot assists a human by pouring water, highlighting advancements in human-robot interaction and dexterous manipulation in everyday environments. <!-- figure, from page 0 (l=0.188,t=0.381,r=0.507,b=0.538), with ID cc2eedd3-fdc3-4400-a6d3-977406107c9e -->\n\nSummary : This photograph shows a robotic arm performing a drawing task on a white plate in a kitchen-like environment, with various household items visible in the background.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm equipped with a gripper holding a pen or marker, positioned over a white plate.\n  ‚Ä¢ The robot appears to be drawing or writing on the plate.\n  ‚Ä¢ The setting resembles a kitchen or laboratory, with a white countertop and several household items (e.g., a box of Twinings tea, a wrapped package, a plate with a cookie or biscuit, a glass jar, and a teapot) visible in the background.\n  ‚Ä¢ The perspective is at table height, focusing on the robot's end effector and the plate.\n  ‚Ä¢ Lighting is even and natural, with neutral colour tones.\n\nTechnical Details :\n  ‚Ä¢ No scale bar or explicit measurement references are visible.\n  ‚Ä¢ No on-image UI elements or text overlays, except for the caption \"Drawing\" below the image.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic arm is on the right side of the image, extending toward the center.\n  ‚Ä¢ The plate being drawn on is in the foreground, slightly left of center.\n  ‚Ä¢ Background items are arranged along the back edge of the countertop, providing context but not obstructing the main subject.\n\nAnalysis :\n  ‚Ä¢ The image visually documents a robot performing a fine-motor task (drawing), likely as part of a demonstration of robotic dexterity or kitchen assistance.\n  ‚Ä¢ The presence of food items and kitchenware suggests the robot may be intended for domestic or culinary applications. <!-- figure, from page 0 (l=0.509,t=0.387,r=0.822,b=0.542), with ID 9ab71824-4c77-4b96-80a9-9886e3bed3a8 -->\n\nFig. 2| Kinova robot in action. a‚Äìf, Action shots of the Kinova Gen3 robot preparing coffee (a‚Äìe) and decorating a plate (f). <!-- text, from page 0 (l=0.061,t=0.542,r=0.716,b=0.559), with ID b4b402d7-886a-499e-8eea-7b9627711662 -->\n\nto generate code and execute actions with force and vision feedback,\neffectively providing the robot with a form of intelligence. Our meth-\nodology was successful in creating a custom GPT-4 (refs. 54,55) with a\ncomprehensive database of flexible motion examples. The database\nsuccessfully incorporated pouring, scooping, drawing, handovers,\npick and place, and opening doors. <!-- text, from page 0 (l=0.061,t=0.588,r=0.500,b=0.671), with ID a3e9000d-75e9-46bc-83cf-afb890cca19d -->\n\nWe found that the robot could identify and extract relevant examples for the downstream task using RAG. We explored various approaches to determine how intelligent machines could make the best use of RAG via our framework. These approaches included customizable open-source methods, such as Haystack56 and Vebra57, as well as proprietary technologies such as Azure Cloud AI. We found that all of these approaches were viable. In our experiment, we chose the simplest method: logically organizing our curated knowledge base in a markdown file and uploading it to the custom GPT API via the ‚ÄòKnowledge‚Äô feature in the GPT‚Äôs platform. This allowed the platform to automatically handle the retrieval processes and select between semantic search (returning relevant text chunks) or document review (providing complete documents or sections from larger texts). We chose this solution as it provided a state-of-the-art embedder and model, gave ease of use and was able to consistently produce good performance in our task. However, our framework allows the incorporation of a range of RAG techniques and ensures that the ‚Äòintelligent robot‚Äô is able to efficiently complete complex tasks. The curated knowledge base, combined with RAG, allowed the language model to access a large selection of low- and high-order functions, each with <!-- text, from page 0 (l=0.062,t=0.672,r=0.500,b=0.944), with ID 0ab3ba03-edd0-4c41-ad99-f612eef6526e -->\n\nknown uncertainties. Our tests showed that this capability enabled\nthe robot to handle numerous scenarios effectively. <!-- text, from page 0 (l=0.510,t=0.589,r=0.947,b=0.618), with ID f63bce43-5c06-45cc-a48c-fc49dcd0dad6 -->\n\nCompleting a complex task  \nThe robot was found to skilfully execute the high-level task specified by the user and was able to access a comprehensive motion primitive database. The database included a variety of flexible examples of specific motions and these were successfully carried out by the robotic arm (Fig. 2). Included in the database were examples of pouring liquids; scooping powders; opening doors with unknown mechanisms; picking up and placing objects; drawing any requested shape; conducting handovers; and moving in various directions, orientations or relative to specified objects. The robot was able to replicate and adapt the motions needed to execute the complex tasks requested by the user. The system enabled the robot to dynamically adjust to environmental variables and uncertainties. This enhanced the robot‚Äôs effectiveness in unpredictable conditions, and improved its flexibility and adaptability in the real-world setting. <!-- text, from page 0 (l=0.510,t=0.629,r=0.948,b=0.835), with ID 88b66330-4499-4fd0-8160-47557de45d0a -->\n\nZero-shot pose detection  \nWe found that an Azure Kinect DK Depth Camera, set to a resolution of 640 √ó 576 px¬≤ with a sample rate of 30 fps for depth sensing, was able to provide sufficient visual input for our method. We achieved calibration using a 14-cm AprilTag, and found that this allowed alignment between the camera and the robot‚Äôs base to an accuracy of less than 10‚Åª‚Å∂. This setup enabled accurate object position detection within <!-- text, from page 0 (l=0.510,t=0.846,r=0.947,b=0.945), with ID c8954f0e-2f7a-41c9-ba4c-6eb9d0e4355b -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.063,t=0.960,r=0.451,b=0.976), with ID 5bc68377-9bca-4693-9b53-17c26e6d0434 -->\n\n594 <!-- marginalia, from page 0 (l=0.915,t=0.962,r=0.947,b=0.976), with ID f4dccf66-347a-40fc-b70f-af1fe97a9904 -->\n\nArticle <!-- marginalia, from page 0 (l=0.061,t=0.025,r=0.127,b=0.045), with ID fefc7fb5-2128-465d-ace2-bea2489f9c4e -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.648,t=0.025,r=0.948,b=0.046), with ID 9ee69797-edb3-45c2-ae71-7626722ce601 -->\n\nSummary : This figure shows a robotic vision detection module in action, where a robot arm is presented with a tabletop scene containing various objects. The system uses zero-shot vision detection to identify and localize a hand, a white mug, and a black kettle, each with associated confidence scores, and extracts target poses for robotic grasping.\n\nphoto:\nScene Overview :\n  ‚Ä¢ The main subject is a robotic arm (labeled \"KINOVA\") positioned at the right side of a table, reaching toward objects.\n  ‚Ä¢ The tabletop contains a white mug, a block-shaped black kettle, a hand, a plate, a pastry, a brown box, and other miscellaneous items.\n  ‚Ä¢ The perspective is an overhead and slightly angled view, showing both the robot and the objects it interacts with.\n  ‚Ä¢ Lighting is even, with a neutral color palette dominated by whites and grays.\n\nTechnical Details :\n  ‚Ä¢ On-image UI elements include colored bounding boxes and labels with confidence scores:\n    ‚Äì \"Hand 0.53\" (red box, left side of image)\n    ‚Äì \"White mug 0.70\" (purple box, near the hand)\n    ‚Äì \"Block kettle 0.79\" (red box, center of table)\n  ‚Ä¢ No scale bar or explicit measurement units are present.\n  ‚Ä¢ The robot arm is equipped with a gripper, poised to interact with the objects.\n\nSpatial Relationships :\n  ‚Ä¢ The hand and white mug are close together on the left side of the table.\n  ‚Ä¢ The block kettle is centrally located, with the robot arm approaching it.\n  ‚Ä¢ The robot arm is on the right, extending toward the objects, suggesting imminent grasping.\n  ‚Ä¢ The bounding boxes and labels are overlaid directly on the corresponding objects for clear identification.\n\nAnalysis :\n  ‚Ä¢ The vision detection module successfully identifies and localizes multiple objects of interest (hand, white mug, block kettle) with confidence scores above 0.5.\n  ‚Ä¢ The spatial arrangement suggests the robot is targeting the block kettle for grasping, as it is closest to the robot's gripper.\n  ‚Ä¢ The system demonstrates the ability to extract actionable information (object identity and pose) for robotic manipulation in a cluttered environment. <!-- figure, from page 0 (l=0.059,t=0.059,r=0.495,b=0.324), with ID 8af71d35-7e80-42e7-a28c-43487aa5ba1a -->\n\nthe scene. Grounded-Segment-Anything<sup>58</sup> was successfully deployed for our language-to-vision module. <!-- text, from page 0 (l=0.061,t=0.358,r=0.499,b=0.385), with ID 09fa7230-b384-4778-8e2f-1f499fb8e693 -->\n\nThe vision system generated a three-dimensional (3D) Voxel\nrepresentation that was effective at identifying object poses in our\nsetup (the used Grounding DINO detection module achieved an aver-\nage precision of 52.5 on the COCO zero-shot transfer benchmark). For\nexample, we found that the module was able to correctly identify the\nwhite cup we used 100% of the time under our experimental conditions. <!-- text, from page 0 (l=0.062,t=0.386,r=0.498,b=0.466), with ID dafe7e76-3eea-45ad-b344-8733fd7069d6 -->\n\nThe 3D Voxel representation contained the meshes of various objects. From these meshes, target poses were extracted at a frequency of 1/3 Hz. In principle, the system should have been able to detect any object. In the pilot work, however, we established that the system would not always accurately identify the different objects associated with making hot beverages. This was often due to confusion between objects with similar shapes or objects absent from the training dataset. We also found that occlusion caused by the robot‚Äôs end-effector could sometimes result in inaccuracies in object detection and lead to errors when we used highly cluttered environments. For example, the mean successful identification rate for a white cup was ~90% at occlusion ratios between 20% and 30%, but decreased substantially at higher occlusion ratios (for example, to ~20% for occlusion ratios between 80% and 90%). We anticipate that improvements in computer vision will enhance the ability of robots to deal with even the most visually complex environments. However, the performance of the vision system was impressive, and we found that our system could cope well with relatively unconstrained environments if the identified issues (for example, using out-of-distribution objects) were avoided (Fig. 3). <!-- text, from page 0 (l=0.063,t=0.468,r=0.498,b=0.727), with ID edb80d8d-2da7-40e2-beab-90e3f36ec2c3 -->\n\nForce feedback  \nWe found that an ATI multiaxis force and torque sensor provided sufficient force feedback for skilful object interaction. The sensor provided six components of force and torque, and the forces exerted by the robot‚Äôs end-effector during task execution were successfully measured. We found that the sensor‚Äôs accuracy was within -2% of the full scale at a sampling rate of 100 Hz. <!-- text, from page 0 (l=0.063,t=0.739,r=0.499,b=0.833), with ID a07925ac-f396-4354-a1f9-50ad77caac59 -->\n\nThe robot was found to demonstrate a variety of motion dynamics\naccompanied by distinct types of force feedback during task execution.\nFigure 4 illustrates the forces experienced as the robot was preparing\ncoffee and handing over a pen. As shown in Fig. 4, a diverse spectrum\nof external forces was handled across various tasks. For example, when\nputting down a mug, the peak upward force was used as an indicator\nof successful placement. By contrast, during drawer manipulation, the\nforces and torques along the x and y axes were critical, highlighting <!-- text, from page 0 (l=0.064,t=0.834,r=0.498,b=0.944), with ID 3795a2a5-6152-4740-9c39-0450a1d3b6a4 -->\n\ntheir importance for successful task execution. The variability in force\nfeedback exemplifies the advantages of our scalable approach that\nadapts to the requirements of diverse motions. <!-- text, from page 0 (l=0.513,t=0.059,r=0.946,b=0.100), with ID 074dd0a5-6064-4839-83ac-803c141bb43d -->\n\nThe pouring accuracy achieved was -5.4 g per 100 g at a pitch\nvelocity of 4 m s‚Åª¬π. We assumed a quasi-static equilibrium to estimate\nthe volume of water poured at any given moment. However, as the pitch\nvelocity increased, the accuracy decreased, with errors approaching\n~20 g s‚Åª¬π at a pitch velocity of 30 m s‚Åª¬π. This decrease in accuracy can\nbe attributed to the breakdown of the quasi-static assumption and the\nimpact of the mass distribution of both pouring medium and container\non measurement accuracy. <!-- text, from page 0 (l=0.511,t=0.101,r=0.946,b=0.209), with ID b635efbf-a57f-4d76-9927-9e6c75bd8ae3 -->\n\nGenerating art  \nDALL-E<sup>59</sup> was found to successfully produce an image from which we could derive a drawing trajectory. It was found that this enabled the robot to draw any design specified by the user. We found that DALL-E was able to create silhouettes based on keywords extracted from the user, such as ‚Äòrandom bird‚Äô or ‚Äòrandom plant‚Äô. The silhouette‚Äôs outline was extracted and transformed to match the dimensions of the target surface. This allowed the robot to replicate the design on various physical objects (Fig. 5). We found that force feedback applied an even pen pressure when drawing, and this allowed control over the z component (Supplementary Section 2). <!-- text, from page 0 (l=0.512,t=0.223,r=0.946,b=0.373), with ID db23817a-0619-43c5-9e68-bb04cfd1280f -->\n\nEvaluation\nWe evaluated our method for generating robotic plans against Vox-Poser, which does not utilize RAG or force feedback. To compare the methods, we prompted an LLM to generate 80 human-like queries, reflecting the range of tasks specified in the knowledge base. These queries were then used to generate robot plans. We compared the performance outcomes from using RAG (our method)‚Äîin which the knowledge base is dynamically integrated into the LLM‚Äôs decision-making process‚Äîto a baseline (VoxPoser) in which the knowledge base was statically incorporated into the LLM‚Äôs context window. It is important to note that the second approach lacks scalability and becomes impractical as the knowledge base expands. <!-- text, from page 0 (l=0.512,t=0.387,r=0.946,b=0.547), with ID 717b59ec-5822-47ca-aeee-63a3bcfa3bf4 -->\n\nWe evaluated the results based on answer faithfulness, which assesses an answer‚Äôs truthfulness and accuracy (ensuring factual representation without fabrication or ‚Äòhallucination‚Äô errors). In our findings, using RAG improved the faithfulness of responses. For GPT-4 (gpt-4-0613), the faithfulness score increased from 0.74 to 0.88 with RAG. Similarly, GPT-3.5-turbo (gpt-3.5-turbo-0125) achieved 0.86 with RAG compared with 0.78 without it, and Zephy-7B-beta saw an increase <!-- text, from page 0 (l=0.513,t=0.549,r=0.945,b=0.644), with ID 2786a20c-fd64-42c2-a7c3-e45ec801c04c -->\n\nSummary : This figure presents time-series plots of force (N), position (m), and velocity (m/s) feedback along the x, y, and z axes during a robot's coffee preparation task, segmented by different motion phases.\n\nline plot:\n# Title & Axes :\n  ‚Ä¢ Title: \"Force, velocity and position feedback. Force (N), velocity (m s‚Åª¬π) and position (m) plots during the robot‚Äôs coffee preparation, illustrating the diverse force feedback across the different motions.\"\n  ‚Ä¢ X-axis: Time (no explicit units or tick labels shown; time increases left to right).\n  ‚Ä¢ Y-axes: \n    ‚Äì Top panel: Force (N), range approximately -30 to +30.\n    ‚Äì Middle panel: Position (m), range approximately -0.8 to +0.8.\n    ‚Äì Bottom panel: Velocity (m/s), range approximately -0.75 to +0.75.\n  ‚Ä¢ No explicit tick labels shown for time; y-axis tick marks are visible but not numerically labeled.\n\n# Data Series :\n  ‚Ä¢ Three variables plotted in each panel, corresponding to spatial axes:\n    ‚Äì x axis: black line\n    ‚Äì y axis: blue line\n    ‚Äì z axis: red line\n\n# Design Encodings :\n  ‚Ä¢ Colour coding: black (x), blue (y), red (z).\n  ‚Ä¢ Three vertically stacked panels: force (top), position (middle), velocity (bottom).\n  ‚Ä¢ Background shading segments the timeline into seven labelled phases:\n    ‚Äì Open door 1 (white)\n    ‚Äì Open door 2 (light blue)\n    ‚Äì Put mug down (light green)\n    ‚Äì Scoop coffee (light pink)\n    ‚Äì Empty in cup (light yellow)\n    ‚Äì Pour water (light orange)\n    ‚Äì Pen handover (light purple)\n  ‚Ä¢ Legend at top right for axis colours.\n  ‚Ä¢ No markers; all lines are solid.\n\n# Motion Phases (Background Segments) :\n  ‚Ä¢ Open door 1\n  ‚Ä¢ Open door 2\n  ‚Ä¢ Put mug down\n  ‚Ä¢ Scoop coffee\n  ‚Ä¢ Empty in cup\n  ‚Ä¢ Pour water\n  ‚Ä¢ Pen handover\n\n# Distribution & Trends :\n  ‚Ä¢ Force: Large oscillations in x and y axes during \"Open door 2\"; force stabilises in later phases.\n  ‚Ä¢ Position: Gradual changes, with more pronounced shifts during \"Put mug down\" and \"Empty in cup\".\n  ‚Ä¢ Velocity: Spikes and oscillations align with transitions between motion phases, especially during \"Open door 2\" and \"Put mug down\".\n\n# Analysis :\n  ‚Ä¢ The robot experiences the most dynamic force and velocity feedback during \"Open door 2\", as shown by high-frequency oscillations in the x and y axes.\n  ‚Ä¢ Position changes are more gradual, with step-like transitions at phase boundaries.\n  ‚Ä¢ The z axis (red) shows distinct force and position changes during \"Scoop coffee\" and \"Empty in cup\", indicating vertical movements.\n  ‚Ä¢ Each motion phase is associated with a unique pattern of force, position, and velocity feedback, reflecting the physical demands of each task segment.\n  ‚Ä¢ The drawing component is omitted for clarity, as noted in the caption. <!-- figure, from page 0 (l=0.509,t=0.671,r=0.941,b=0.942), with ID 0e32d04f-0157-43e4-beb3-4761f3c6df67 -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.064,t=0.960,r=0.449,b=0.976), with ID 1e7f8a3f-659e-4151-8d58-34d0625730ac -->\n\n595 <!-- marginalia, from page 0 (l=0.917,t=0.963,r=0.948,b=0.976), with ID e2678a97-5582-485b-b36e-80afc645503d -->\n\nArticle <!-- marginalia, from page 0 (l=0.061,t=0.026,r=0.127,b=0.045), with ID 64b23470-8bba-4e08-b069-83b390627b3d -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.650,t=0.026,r=0.949,b=0.046), with ID d212b5cb-9caa-4371-8085-4d3b28e720c0 -->\n\nSummary : This figure illustrates a three-step drawing process for generating and physically rendering images of a random animal, food, and plant. Each row shows (1) the generated image, (2) a contour plot with drawing path, and (3) the final drawing on a physical plate.\n\nphoto and process diagram:\n# Overview :\n  ‚Ä¢ Three rows, each corresponding to a different category: random animal (bird), random food (apple), and random plant (clover).\n  ‚Ä¢ Each row contains three columns: generated image, contour plot of drawing path, and photo of the drawing rendered on a plate.\n\n# Row 1: Random Animal (Bird) :\n  ‚Ä¢ Left: Black silhouette of a bird in flight, facing right.\n  ‚Ä¢ Middle: Contour plot with grid and numbered points tracing the outline of the bird, showing the drawing sequence.\n  ‚Ä¢ Right: Photo of a plate with a green line drawing of the bird, matching the generated image's outline.\n\n# Row 2: Random Food (Apple) :\n  ‚Ä¢ Left: Black silhouette of an apple with a leaf.\n  ‚Ä¢ Middle: Contour plot with grid and numbered points tracing the apple's outline, including the leaf and stem.\n  ‚Ä¢ Right: Photo of a plate with a red line drawing of the apple, closely following the generated image.\n\n# Row 3: Random Plant (Clover) :\n  ‚Ä¢ Left: Black silhouette of a three-leaf clover.\n  ‚Ä¢ Middle: Contour plot with grid and numbered points tracing the clover's outline, including the stem and three leaves.\n  ‚Ä¢ Right: Photo of a plate with a blue line drawing of the clover, matching the generated image.\n\n# Technical Details :\n  ‚Ä¢ All generated images are black silhouettes on a white background.\n  ‚Ä¢ Contour plots use purple lines and numbered points to indicate drawing order and path.\n  ‚Ä¢ Plate drawings use single-color lines (green, red, blue) corresponding to each category.\n  ‚Ä¢ Plates are photographed from above, with the drawing centered.\n\n# Spatial Relationships :\n  ‚Ä¢ Each row presents a left-to-right sequence: digital image ‚Üí drawing path ‚Üí physical rendering.\n  ‚Ä¢ The physical drawings on plates closely match the contours of the generated images.\n\n# Analysis :\n  ‚Ä¢ The process demonstrates high fidelity between the generated image, the planned drawing path, and the final physical rendering.\n  ‚Ä¢ The contour plots reveal the sequential logic used to trace each shape.\n  ‚Ä¢ The approach is consistent across different object categories (animal, food, plant), indicating a generalizable drawing method.\n  ‚Ä¢ The physical renderings on plates are clear and accurately follow the digital outlines, validating the effectiveness of the drawing process. <!-- figure, from page 0 (l=0.065,t=0.063,r=0.946,b=0.588), with ID 8848d972-5082-4539-ad73-75c0ba929387 -->\n\nfrom 0.37 to 0.44. The improvement in faithfulness is particularly key\nfor robotic applications, where accurate execution during physical\ninteractions is essential. <!-- text, from page 0 (l=0.062,t=0.615,r=0.499,b=0.659), with ID 020a0d3c-bbdf-4e40-a70d-37c9fb631251 -->\n\n## Discussion\nWe tested our methodology‚Äîthe ELLMER framework‚Äîthat combines techniques from artificial intelligence and robot manipulation to create an intelligent robot. Our approach successfully combined the cognitive abilities of LLMs with the sensorimotor skills of robots, enabling our robot to interpret a high-order verbal command and execute a complex long-horizon task while adeptly managing uncertainties. We used the LLM, augmented with feedback loops and RAG, to write expressive code and facilitate the manipulation sub-tasks required by the robot to achieve the high-level goal (making a hot beverage). ELLMER allowed real-time adaptation to environmental changes and leveraged a repository of precise solutions via RAG. This ensured accurate task execution and broad adaptability32. <!-- text, from page 0 (l=0.063,t=0.668,r=0.499,b=0.847), with ID a783fd90-c2de-457f-915f-89ca154b23ff -->\n\nELLMER encoded known constraints into the code examples (‚Äòmotion functions‚Äô) and enabled rapid accommodation to numerous uncertainties, such as fluctuating ingredient quantities or opening unknown drawers‚Äîcapabilities that other methods lack without extensive additional training29,33,60,61. The integration of vision, force and language modalities enhanced the manipulation performance. The force sensors improved task precision (for example, pouring a precise <!-- text, from page 0 (l=0.064,t=0.848,r=0.498,b=0.943), with ID 689dbe52-13e8-4435-a645-6c64a43b8b66 -->\n\nand accurate amount of liquid when vision was occluded), whereas the\nvision system identified object positions and movements. The language\ncapabilities enabled the system to produce feedback within the code,\nwhich is critical for adjusting to new tasks. The curated knowledge base\nimproved the LLM‚Äôs performance by tailoring information retrieval to\nthe specific task specifications, and this ensured high-quality contextu-\nally relevant outputs. A curated knowledge base is a pragmatic element\nthat enhances controllability, accuracy and scalability. In this context,\nRAG can be seen as providing a cultural milieu of knowledge from which\na robot can draw. In particular, this mirrors the ‚Äòintelligence‚Äô afforded\nto humans through the cultural transmission of knowledge. Thus, our\nwork shows that integrating advanced language models and sensori-\nmotor control strategies allows robots to leverage the exponential\nadvancements in LLMs, enabling more sophisticated interactions.\nThis will usher in the next age of automation with unprecedented levels\nof autonomy and precision, accentuating the need to manage these\nadvancements safely62. <!-- text, from page 0 (l=0.511,t=0.618,r=0.947,b=0.847), with ID 31b0e1b3-7317-4355-b5d0-706dcab26d22 -->\n\nELLMER‚Äôs potential extends to creating intricate and artistic movements. For instance, a model like DALL¬∑E allows trajectories to be derived from visual inputs and opens new avenues for robotic trajectory generation. This method can be widely applied in tasks such as cake decoration or latte art. In future work, incorporating queries and images will enable novel trajectory generation, allowing for increased versatility. Moreover, recent LLM enhancements are <!-- text, from page 0 (l=0.512,t=0.848,r=0.946,b=0.944), with ID 9c88aa9c-83d9-4920-9be6-2503a2304b90 -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.064,t=0.960,r=0.450,b=0.976), with ID b19cd3b2-32a8-400a-8ae2-adf32528b295 -->\n\n596 <!-- marginalia, from page 0 (l=0.915,t=0.962,r=0.948,b=0.976), with ID 86537c44-94d0-4ad7-ba3a-f539ec5114f9 -->\n\nArticle <!-- marginalia, from page 0 (l=0.061,t=0.026,r=0.127,b=0.045), with ID 69d7ac0d-b604-46e3-b2c7-44914609519a -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.648,t=0.025,r=0.948,b=0.046), with ID 9e7245df-4468-4698-bfc3-b1c355f78906 -->\n\nSummary : This photograph shows a Kinova Gen3 robotic arm in the process of preparing coffee and decorating a plate, as part of a demonstration of robotic manipulation in a kitchen-like environment.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a Kinova Gen3 robotic arm positioned on the right side of a white table.\n  ‚Ä¢ The robot is holding a black-handled tool or utensil, interacting with a plate that has a pastry and a cup of coffee.\n  ‚Ä¢ The setting resembles a kitchen or laboratory workspace, with a coffee maker, a set of drawers, and various containers (including a jar of instant coffee, a sugar jar, and a cup with stirring sticks).\n  ‚Ä¢ The lighting is bright and even, with a neutral colour palette dominated by white, black, and metallic tones.\n\nTechnical Details :\n  ‚Ä¢ No scale bar or magnification is present.\n  ‚Ä¢ The plate on the table has a blue line drawing of a bird as decoration.\n  ‚Ä¢ The coffee cup is white with a blue rim, and the pastry is placed on a purple plate with a white pattern.\n  ‚Ä¢ The robot is a Kinova Gen3, identifiable by its white casing and blue logo.\n\nSpatial Relationships :\n  ‚Ä¢ The robot is on the right, reaching toward the center of the table.\n  ‚Ä¢ The coffee maker and drawers are on the left side of the image.\n  ‚Ä¢ The decorated plate and coffee cup are centrally placed, with other items (jars, utensils) arranged nearby.\n\nAnalysis :\n  ‚Ä¢ The image visually demonstrates the robot‚Äôs ability to perform multi-step tasks such as preparing coffee and decorating a plate, highlighting dexterity and workspace organization. The scene is staged to showcase both the robot and the completed food items, emphasizing the integration of robotics into everyday kitchen activities. <!-- figure, from page 0 (l=0.060,t=0.058,r=0.499,b=0.275), with ID 31b848d5-0a8e-4612-8d7f-b387660b1e61 -->\n\nset to notably improve the fluidity and effectiveness of human‚Äìrobot\ninteractions. Our examples of coffee making and plate decoration\nrepresent only a subset of the complex task types that a sophisticated\nrobot might be required to undertake. ELLMER is conducive to being\nscaled up, so it includes a wide range of possible long-horizon tasks.\nThus, ELLMER could incorporate a database of feedback loops or\n‚Äòlearning-from-demonstration‚Äô examples to facilitate a wide variety\nof complex robotic manipulations. <!-- text, from page 0 (l=0.060,t=0.303,r=0.500,b=0.413), with ID 293ffbb0-cdb3-4a66-a1fc-975294ccccea -->\n\nELLmER is based on two assumptions concerning computer vision: (1) the vision module accurately identifies and classifies objects within the scene and (2) a comprehensive affordance map of the utensils is available. We endowed our model with prior knowledge of the kettle, spoon and door handle affordances, but recent work suggests that affordances can be learned with minimal data63,64. Our focus was not on object detection, but we noted that detection response times hindered optimal performance. In addition, ELLMER could adjust to real-time changes but struggled with proactive adaptations (for example, task switching midway without prior programming). In future iterations, more frequent querying of the language model would allow the reassessment and modification of overall plans based on new inputs. We also note that there are still challenges that need to be addressed, such as the sophisticated modelling of complex force dynamics (for example, the forces on the end-effector as a function of the flow rate, container size and liquid viscosity) and the integration of spatial awareness tools (such as OctoMaps, a robotic library for a 3D occupancy map). Incorporating tactile sensors and using soft robotic techniques would improve the robot‚Äôs ability to apply appropriate forces without causing damage. ELLMER provides a flexible platform for incorporating these research developments, enabling robots to use ‚Äòsensory‚Äô feedback to interpret material properties and precisely tailor the forces they apply. <!-- text, from page 0 (l=0.061,t=0.414,r=0.499,b=0.711), with ID 4a876d32-6323-423f-8e1e-c29dc47e0e88 -->\n\nThe current iteration of ELLMER allowed the robot to successfully\ncomplete a complex task in ‚Äòone shot‚Äô. This provides a compelling\npicture of the capabilities of intelligent machines that combine sen-\nsorimotor capabilities with the abstract reasoning provided by LLMs.\nNevertheless, we anticipate that the robot capacity will increase expo-\nnentially as the components combined within ELLMER become ever\nmore refined. Our framework is hardware agnostic and can be easily\ncustomized with open-source RAG solutions like Haystack, supporting\nquick adjustments to embedders, retrievers, chunking techniques and\nLLMs. ELLMER offers a flexible framework for researchers to collabora-\ntively develop intelligent machines. Supplementary Section 3 provides\nmore information on ELLMER and future research. <!-- text, from page 0 (l=0.063,t=0.712,r=0.498,b=0.874), with ID 37a45eaa-92b1-4b84-ab0e-39f632ee8b31 -->\n\nThe power of our approach lies in the embodiment of cognition through a framework that combines enhanced sensorimotor abilities with the cognitive reasoning capabilities of LLMs. Through this combination, ELLMER enables robots to explore and interact with their environment more effectively, emulating aspects of the connection between <!-- text, from page 0 (l=0.064,t=0.875,r=0.498,b=0.943), with ID ab51da6f-0c94-459c-8932-619c12cb3e0d -->\n\nexperience and action observed in human intelligence. This opens up\nopportunities for robots to gain a form of ‚Äòphysical intelligence‚Äô, where\ntheir exploration of the environment drives the sensorimotor learn-\ning process. In conclusion, ELLMER integrates language processing,\nRAG, force and vision to enable robots to adapt to complex tasks. It\ncombines the following features: (1) interpreting high-level human com-\nmands, (2) completing long-horizon tasks and (3) utilizing integrated\nforce and vision signals to manage noise and disturbances in changing\nenvironments. ELLMER allows methods such as reinforcement learn-\ning, imitation learning and flexible motion primitives to be combined\nholistically for enhanced adaptability and ‚Äòrobot intelligence‚Äô in diverse\nand dynamic scenarios. It demonstrates that integrating the cognitive\nreasoning capabilities of LLMs with robots‚Äô sensorimotor skills allows\nthem to interpret and manipulate their environment and complete\ncomplex tasks through embodied machine intelligence. <!-- text, from page 0 (l=0.510,t=0.059,r=0.947,b=0.265), with ID b08855b5-6d96-47ac-96d8-8aaccc0d7a6e -->\n\n## Methods\n### Overview\nThe goal of the robot was to respond to high-level human commands in a dynamic environment, such as a home kitchen. We designed a realistic setting featuring items including a kettle, white mug, drawers, kitchen paraphernalia and a coffee pot. The scenario was designed to test the robot‚Äôs ability to perform diverse tasks in a realistic, although reasonably constrained, environment as it interacts with a human present. We assumed that robotic low-level control mechanisms managed obstacle avoidance. The pipeline consisted of a language-processing component for task execution, a vision system for pose detection and a force module for object manipulation. All of this was integrated within a robotic operating system (ROS) process. <!-- text, from page 0 (l=0.511,t=0.276,r=0.946,b=0.453), with ID dc2eadf8-7410-47c4-8dce-0918907d0f29 -->\n\nSpecifically, our approach built on the ‚Äòcode for dynamic policies‚Äô approach‚Å∂‚Åµ that can facilitate adaptable robotic actions. In our implementation, we utilized GPT-4 and OpenAI‚Äôs RAG infrastructure. We leveraged LLMs‚Äô capabilities using RAG¬≥¬≤ to dynamically select and adapt the most suitable policy from a database or generate its own code based on relevant examples. In contrast to existing pure LLM-driven methods¬≤‚ÅµÀí¬≤‚Å∑Àí¬≤‚Åπ, we integrated force and vision into the framework, allowing the system to adapt to a variety of complex tasks in dynamic settings. This approach equips the robotic system with the capacity for high-level contextual understanding¬≤‚Åµ and the proficiency to execute complex tasks with real-time feedback, ensuring accuracy and precision. The approach ensures that each action is aligned with the specific demands of the task and the environmental conditions (Fig. 6). <!-- text, from page 0 (l=0.511,t=0.454,r=0.946,b=0.631), with ID e6826367-2276-491a-a4f5-254487726f58 -->\n\nHardware and software\nA Kinova seven-degrees-of-freedom robot was used. An Azure Kinect Sensor was used at a resolution of 640 √ó 576 px¬≤ and 30 fps, along with an ATI multiaxis force sensor. A140-mm Robotiq gripper was attached to the end of the robot. The force sensor was attached to the Robotiq gripper and Kinova arm using a 3D printed flange. A small cylinder was placed on the force sensor on the side closest to the gripper so that the movements of the gripper would not touch the force sensor, leading to readings being inaccurate. A Dell desktop computer with an Intel Core i9 processor with an NVIDIA RTX 2080 graphics-processing unit was used and connected to the robot with an Ethernet cable. Similarly, both Azure cameras were attached to the desktop. Ubuntu 20.04 and the ROS were used. Our code relied on the Kinova ROS Kortex library. The NVIDIA RTX 2080 utilizes -225 W under typical load conditions‚Å∂‚Å∂, whereas the Kinova robotic arm consumes -36 W (ref. 67). In our scenarios, each task runs for up to 4 min. Utilizing the EPA‚Äôs average conversion factor of -0.4 kg of CO‚ÇÇ per kWh for mixed energy sources‚Å∂‚Å∏, the carbon emission for each task comes to -0.007 kg (7 g) of CO‚ÇÇ. <!-- text, from page 0 (l=0.511,t=0.644,r=0.946,b=0.888), with ID 4aea81a5-e567-4d72-8dec-63a1ef9cff5c -->\n\n**Language processing**\nThe LLM processes an image and the user‚Äôs query, systematically breaking down the complex task $L_T$ into a sequence of steps $\\{L_1, L_2, ..., L_n\\}$, <!-- text, from page 0 (l=0.512,t=0.902,r=0.945,b=0.946), with ID b72e392a-2d29-466d-8880-e9b461881559 -->\n\n[**Nature Machine Intelligence**](https://www.nature.com/natmachintell) | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.064,t=0.960,r=0.450,b=0.977), with ID 04bdb4fe-5734-44d8-a01f-293c7b93d2f9 -->\n\n597 <!-- marginalia, from page 0 (l=0.918,t=0.963,r=0.949,b=0.977), with ID e634a184-f336-4e69-9b5c-2bcc29a3b7eb -->\n\nArticle <!-- marginalia, from page 0 (l=0.062,t=0.026,r=0.126,b=0.044), with ID 1fe33de3-9658-49f9-9f73-fc67584b00e0 -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.649,t=0.026,r=0.948,b=0.045), with ID d22b9f21-232d-4d03-b1bd-ffc63bddb98a -->\n\nwhere each step $L_i$ may depend on the completion of the preceding\nsteps. The sequence of steps is critical, and dependencies exist between\nsteps; for example, if an object (for example, a mug) is required but not\nfound, then potentially a cupboard should be opened. <!-- text, from page 0 (l=0.063,t=0.058,r=0.499,b=0.114), with ID 07984a8d-c977-4340-92a4-dbcdd3234ab2 -->\n\nThe environmental data gathered from the initial image input are key in decomposing the abstract task. For instance, when asked to make a beverage, the ingredients present in the environment are critical in deciding which drink to make, and the visual information can help identify possible locations. The interface was facilitated by GPT-4, which ran under the instruction to write and dispatch code to a robot via the server platform. The process was assisted by a knowledge base containing code examples and allowed continuous communication with the robot. The curated knowledge base contained validated examples of low- and high-order actions that incorporate known uncertainties. Including these motion examples is key to enabling the robot to handle numerous scenarios and complete long-horizon tasks. High-level motion primitives or policies can compress multiple known uncertainties into a single function, reducing the need for extensive code writing. RAG allowed the knowledge base to be comprehensive without sacrificing performance. The system interacted with the ROS and communicated via a low-latency connection provided by the EC2 server through JSON action queries and responses. <!-- text, from page 0 (l=0.063,t=0.115,r=0.499,b=0.358), with ID e6aaf132-6202-4b42-bd80-88ddf61da2b7 -->\n\nThe dependency among tasks is expressed through conditional probabilities such as $P(L_{2A}, L_{2B}|L_1)$, which specifies the likelihood of progressing to tasks $L_{2A}$ or $L_{2B}$ following the successful execution of task $L_1$. This helps in planning the sequence of steps, ensuring the robot can adapt its actions based on real-time feedback. The LLM generates executable code that is sent to the server, based on the instructions (prompt) and a knowledge base containing examples. The code is run on the ROS in a secure environment that only has access to predefined functions, thereby ensuring safety in task execution. <!-- text, from page 0 (l=0.063,t=0.359,r=0.498,b=0.482), with ID 21a150d0-1954-40cb-afc8-8d2863c587ac -->\n\n**RAG**\nA key feature of our system is the deployment of RAG. RAG integrates user queries with information from a continually updated, curated knowledge base, optimizing the output of the LLM. This approach allows the model to follow code examples provided in the database, ensuring accuracy, reliability and scalability as the knowledge base evolves. <!-- text, from page 0 (l=0.063,t=0.494,r=0.498,b=0.576), with ID 01bc99fe-7ffe-41d3-8589-4cf3a60ad9b5 -->\n\nWe used vector RAG, which involves using an encoder to embed\nthe query ($q$) and segments of the knowledge base ({$s_1, s_2, ..., s_m$}), known\nas chunks, into vector representations. Chunks were then compared\nwith the query based on cosine similarity, and the top $k$ chunks were\nselected as contextually relevant information for generating responses.\nAlternative retrieval techniques that can be used within our framework\ninclude traditional RAG (keyword-/rule-based RAG) or hybrid retrieval\nmethods. <!-- text, from page 0 (l=0.064,t=0.576,r=0.497,b=0.684), with ID 8eb33839-3d90-4062-9994-b2905455ec7f -->\n\nThe RAG pipeline can be customized by selecting different document stores (the medium in which the knowledge base is stored and organized). In our experimental test, we used the inbuilt OpenAI RAG process and organized our curated knowledge base in a markdown file as the document store. However, a range of other RAG approaches can be used in our framework, utilizing tools like Haystack^56 and Vebra^57. These tools allow users to select a range of document stores‚Äîfrom ‚Äòmarkdown files‚Äô for simple text-based knowledge to ‚ÄòElasticsearch‚Äô for complex, indexed data‚Äîalong with specific embedders, retrievers and chunking techniques, as well as the LLM itself. <!-- text, from page 0 (l=0.063,t=0.685,r=0.497,b=0.821), with ID 147e6e78-dcce-47bd-8ac0-b05760426c96 -->\n\nVision system  \nGrounded-Segment-Anything was used as the language-to-vision model to create a 3D voxel that highlighted the positions of all objects and allowed their poses to be extracted for robotic grasping$^{58,69}$. This enabled (1) the generation of object-specific bounding boxes, (2) the manufacture of segmented masks via MobileSAM and (3) the creation of voxels that encapsulate the detected objects. The voxels allowed target object poses to be extracted. <!-- text, from page 0 (l=0.064,t=0.834,r=0.498,b=0.943), with ID fabb5d25-5a65-4afa-bab7-57c48fd5570a -->\n\nForce module\nTo ensure accurate measurements in force-rich applications, we calibrated the ATI force sensor to compensate for gravitational forces, ensuring it registered zero in the absence of external forces. This calibration is key for accurately predicting the external forces applied to the end-effector. The process involved sequentially zeroing the force sensor on one axis, rotating the sensor and then zeroing on the next axis. The local forces were transformed into the global plane to estimate the upward force at different rotations $F_{global} = T_{end\\ effector\\ to\\ robot\\ base} \\times$ <!-- text, from page 0 (l=0.513,t=0.060,r=0.945,b=0.181), with ID 7f514b1c-a80c-4b33-a920-1178ccd67a76 -->\n\n$_{global}^{T}end\\_effector\\_to\\_robot\\_base F_{local}$, where $F_{global}$ is the force vector in the global (robot base) coordinate frame, $T_{end\\_effector\\_to\\_robot\\_base}$ is the transformation matrix from the end-effector‚Äôs frame to the robot‚Äôs base frame and $F_{local}$ is the force vector in the local coordinate frame of the end-effector. We explored various methods, such as moving the sensor‚Äôs position and orientation and using polynomial functions for calibration. However, the simpler calibration method was found to be the most effective. <!-- text, from page 0 (l=0.513,t=0.178,r=0.944,b=0.277), with ID 98c216cd-a12b-4c53-87ad-8594c5f20bc2 -->\n\nTo estimate the flow rates, we assumed a condition of static equilibrium and maintain slow operational speeds during pouring. Mathematically, this is represented as $F_{up} \\approx mg$ and $\\Delta F_{up} \\approx \\Delta mg$. In situations involving variable acceleration, the relationship between forces and flow rates becomes more complex. It necessitates a dynamic model that accounts for varying inputs, such as flow rates, container‚Äôs centre of mass and inertia of the end-effector, to map dynamic force inputs to the pouring flow rates. <!-- text, from page 0 (l=0.513,t=0.278,r=0.944,b=0.386), with ID a9f568c1-045f-461c-9045-040623ab2123 -->\n\nThe system continuously manages force vectors along three axes,\nadjusting the applied force based on the criteria within its knowledge\nbase. The LLM dynamically selects the necessary force magnitudes and\ndirections tailored to meet specific downstream task requirements. For\nexample, the knowledge base may specify varying force magnitudes to\nbe applied depending on the object characteristics or task demands.\nThis approach enables the system to adjust its actions autonomously\nto align with a broad range of operational criteria. <!-- text, from page 0 (l=0.512,t=0.387,r=0.944,b=0.495), with ID b986bb2b-abcc-4b12-9a94-6c62feb71349 -->\n\nROS operation  \nIn this work, we initiated the robotic processes by launching a Kinova ROS Kortex driver. This established a node that enables communication within the ROS network and the Kinova Gen3 robot. The node publishes several topics that subscribers can access, and it provides services that can be called to modify the robot‚Äôs configuration. The base joints are updated at a frequency of 40 Hz. Concurrently, the Robotiq 2F-140 mm gripper node is activated at 50 Hz. The node sets up a communication link with the gripper via a USB connection, and it initiates an action server that enables precise control of the gripper and facilitates the exchange of operating data. <!-- text, from page 0 (l=0.512,t=0.509,r=0.944,b=0.656), with ID d80ea394-7e94-46af-822a-3d477f742d08 -->\n\nA vital element of our robotic system is the vision module node.\nA ‚Äòclasses‚Äô variable is used to identify the target pose of selected objects\nwithin the environment. This variable can be dynamically updated,\nthereby allowing the system to adapt to changes in the scene. The pose\ncoordinates of the objects, as established by the ‚Äòclasses‚Äô variable, are\npublished approximately at every ~ $\\frac{1}{3}$ Hz. This is largely due to the\nprocessing time of Grounding DINO in detecting objects and establish-\ning the bounding boxes. Moreover, we used an AprilTag to determine\nthe position of the camera relative to the robot‚Äôs base. This is repre-\nsented as $PR = T_{AR} \\times (T_{CA} \\times P^C)$, where $P^C$ is the point in the camera frame,\n$T_{CA}$ is the transformation matrix from the camera frame to the AprilTag,\n$T_{AR}$ is the transformation matrix from the AprilTag to the robot‚Äôs\nbase and PR is the point in the robot‚Äôs base frame. <!-- text, from page 0 (l=0.512,t=0.658,r=0.944,b=0.833), with ID 1f4da0b9-56e8-4bec-a992-8211dc401749 -->\n\nIn parallel, a force node is launched at a frequency of 100 Hz and provides multiaxis force and torque readings, localized to the ATI force transducer. The readings are transformed using a quaternion-based 3 √ó 3 rotation matrix to align with the global base frame of the robot, providing raw and averaged values over the last five time steps across fixed degrees of freedom. It calculates forces in the global frame of the robot base using the rotational matrix, calculated from the kinematic data. <!-- text, from page 0 (l=0.513,t=0.835,r=0.944,b=0.942), with ID 11133c44-208a-4f6a-85f5-f55b7d3197b4 -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.065,t=0.960,r=0.449,b=0.976), with ID 21694e65-ea03-4a48-8915-c8b26098170c -->\n\n598 <!-- marginalia, from page 0 (l=0.915,t=0.963,r=0.947,b=0.976), with ID d1c6e9b7-8e3e-4b73-8f05-303e63ecc135 -->\n\nArticle <!-- marginalia, from page 0 (l=0.061,t=0.025,r=0.127,b=0.045), with ID 4dd34b43-9373-4299-a51f-69744f6ec8c5 -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.648,t=0.025,r=0.949,b=0.045), with ID 887be186-cec8-4c33-a33f-92e14807aeea -->\n\nROS facilitates the continuous processing of multimodal feedback\ndata from the language processing, vision systems, force metrics and\njoint end-effector positions. The motions operate on a foundational\nsix-degrees-of-freedom twist command, which controls velocity and\nthe variable speed and force gripper procedures for opening and clos-\ning. This enables the integration of hard-coded safety constraints, such\nas maximum velocity and force limits, as well as workspace boundaries.\n\nThe linear velocities were clamped within ¬±0.05 m s‚Åª¬π and the\nangular velocities were clamped within ¬±60¬∞ s‚Åª¬π. End-effector forces\nwere also limited to 20 N. This is coded into the fundamental motion\nprimitives; therefore, error in the language model will not override\nthis. The end-effector is also clamped within the predefined workspace\nbounds of x = [0.0, 1.1], y = [‚Äì0.3, 0.3] and z = [0, 1.0]. This is checked in\nfuture time steps by a publisher at a frequency of 10 Hz. <!-- text, from page 0 (l=0.062,t=0.057,r=0.501,b=0.251), with ID 2ef5c1c4-2995-44b0-8a4f-274500d29731 -->\n\n**Data availability**  \nThe dataset used in this work is available in an open-source GitHub repository at https://github.com/ruaidhmon/ELLMER. <!-- text, from page 0 (l=0.062,t=0.260,r=0.498,b=0.306), with ID be3cae3f-2db8-46ef-baa6-6bae8d57cadb -->\n\n**Code availability**  \nThe code supporting this study is available via GitHub at https://github.com/ruairidhmon/ELLMER and has been archived in Zenodo at https://doi.org/10.5281/zenodo.14483539 (ref. 70). <!-- text, from page 0 (l=0.062,t=0.314,r=0.499,b=0.375), with ID 8dd9f139-b678-4f61-ab69-f23a2fb0e1d5 -->\n\n**References**\n1. Intelligence research should not be held back by its past. *Nature* **545**, 385‚Äì386 (2017).\n2. Friston, K. Embodied inference and spatial cognition. *Cogn. Process.* **13**, 497‚Äì514 (2012).\n3. Wilson, M. Six views of embodied cognition. *Psychon. Bull. Rev.* **9**, 625‚Äì636 (2002).\n4. Clark, A. An embodied cognitive science. *Trends Cogn. Sci.* **3**, 345‚Äì351 (1999).\n5. Stella, F., Della Santina, C. & Hughes, J. How can LLMs transform the robotic design process? *Nat. Mach. Intell.* **5**, 561‚Äì564 (2023).\n6. Miriyev, A. & Kovac, M. Skills for physical artificial intelligence. *Nat. Mach. Intell.* **2**, 658‚Äì660 (2020).\n7. Cui, J. & Trinkle, J. Toward next-generation learned robot manipulation. *Sci. Robot.* **6**, eabd9461 (2021).\n8. Arents, J. & Greitans, M. Smart industrial robot control trends, challenges and opportunities within manufacturing. *Appl. Sci.* **12**, 937 (2022).\n9. Billard, A. & Kragic, D. Trends and challenges in robot manipulation. *Science* **364**, eaat8414 (2019).\n10. Yang, G.-Z. et al. The grand challenges of *Science Robotics*. *Sci. Robot.* **3**, eaar7650 (2018).\n11. Buchanan, R., Rofer, A., Moura, J., Valada, A. & Vijayakumar, S. Online estimation of articulated objects with factor graphs using vision and proprioceptive sensing. In *2024 IEEE International Conference on Robotics and Automation (ICRA)* 16111‚Äì16117 (IEEE, 2024).\n12. Nikolaidis, S., Ramakrishnan, R., Gu, K. & Shah, J. Efficient model learning from joint-action demonstrations for human-robot collaborative tasks. In *2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)* 189‚Äì196 (IEEE, 2015).\n13. Saveriano, M., Abu-Dakka, F. J., Kramberger, A. & Peternel, L. Dynamic movement primitives in robotics: a tutorial survey. *Int. J. Robot. Res.* **42**, 1133‚Äì1184 (2023).\n14. Kober, J. et al. Movement templates for learning of hitting and batting. In 2010 *IEEE International Conference on Robotics and Automation* 853‚Äì858 (IEEE, 2010).\n15. Huang, W. et al. VoxPoser: composable 3D value maps for robotic manipulation with language models. In *Proc. 7th Conference on Robot Learning* 540‚Äì562 (PMLR, 2023). <!-- text, from page 0 (l=0.059,t=0.384,r=0.498,b=0.944), with ID b359775e-1706-48e7-84b6-8b147989ccfd -->\n\n16. Zhang, D. et al. Explainable hierarchical imitation learning for\nrobotic drink pouring. In IEEE Transactions on Automation Science\nand Engineering 3871‚Äì3887 (2022).\n17. Hussein, A., Gaber, M. M., Elyan, E. & Jayne, C. Imitation learning:\na survey of learning methods. ACM Comput. Surv. 50, 21:1‚Äì21:35\n(2017).\n18. Di Palo, N. & Johns, E. DINObot: robot manipulation via retrieval\nand alignment with vision foundation models. In International\nConference on Robotics and Automation (ICRA) 2798‚Äì805\n(IEEE, 2024).\n19. Shridhar, M., Manuelli, L. & Fox, D. CLIPort: what and where\npathways for robotic manipulation. In Proc. 5th Conference on\nRobot Learning 894‚Äì906 (PMLR, 2022).\n20. Shridhar, M., Manuelli, L. & Fox, D. Perceiver-Actor: a multi-task\ntransformer for robotic manipulation. In Proc. 6th Conference on\nRobot Learning 785‚Äì799 (PMLR, 2023).\n21. Mees, O., Hermann, L. & Burgard, W. What matters in language\nconditioned robotic imitation learning over unstructured data.\nIEEE Robot. Autom. Lett. 7, 11205‚Äì11212 (2022).\n22. Mees, O., Borja-Diaz, J. & Burgard, W. Grounding language\nwith visual affordances over unstructured data. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA)\n11576‚Äì11582 (IEEE, 2023).\n23. Shao, L., Migimatsu, T., Zhang, Q., Yang, K. & Bohg, J.\nConcept2Robot: learning manipulation concepts from\ninstructions and human demonstrations. Int. J. Robot. Res. 40,\n1419‚Äì1434 (2021).\n24. Ichter, B. et al. Do as I can, not as I say: grounding language in\nrobotic affordances. In Proc. 6th Conference on Robot Learning\n287‚Äì318 (PMLR, 2023).\n25. Driess, D. et al. PaLM-E: an embodied multimodal language\nmodel. In Proc. 40th International Conference on Machine\nLearning 8469‚Äì8488 (PMLR, 2023).\n26. Peng, A. et al. Preference-conditioned language-guided\nabstraction. In Proc. 2024 ACM/IEEE International Conference\non Human-Robot Interaction, HRI ‚Äô24 572‚Äì581 (Association for\nComputing Machinery, 2024).\n27. Huang, W., Abbeel, P., Pathak, D. & Mordatch, I. Language models\nas zero-shot planners: extracting actionable knowledge for\nembodied agents. In Proc. 39th International Conference on\nMachine Learning 9118‚Äì9147 (PMLR, 2022).\n28. Huang, J. & Chang, K. C.-C. Towards reasoning in large language\nmodels: a survey. In Findings of the Association for Computational\nLinguistics: ACL 2023 1049‚Äì1065 (Association for Computational\nLinguistics, 2023).\n29. Zitkovich, B. et al. RT-2: vision-language-action models transfer\nweb knowledge to robotic control. In Proc. 7th Conference on\nRobot Learning 2165‚Äì2183 (PMLR, 2023).\n30. Ma, X., Patidar, S., Haughton, I. & James, S. Hierarchical diffusion\npolicy for semantics-aware multi-task robotic manipulation.\nIn Proc. IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) 18081‚Äì18090 (IEEE, 2024).\n31. Zhang, C., Chen, J., Li, J., Peng, Y. & Mao, Z. Large language\nmodels for human-robot interaction: a review. Biomimetic Intell.\nRobot. 3, 100131 (2023).\n32. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks. In Advances in Neural Information Processing\nSystems 9459‚Äì9474 (Curran Associates, 2020).\n33. Raiaan, M. et al. A review on large language models:\narchitectures, applications, taxonomies, open issues and\nchallenges. IEEE Access 12, 26839‚Äì26874 (2024).\n34. Rozo, L., Jimenez, P. & Torras, C. Force-based robot learning of\npouring skills using parametric hidden Markov models. In 9th\nInternational Workshop on Robot Motion and Control 227‚Äì232\n(IEEE, 2013). <!-- text, from page 0 (l=0.510,t=0.058,r=0.944,b=0.942), with ID f5520add-2b55-4515-9741-f466e3a09b98 -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.063,t=0.959,r=0.451,b=0.977), with ID dc78ffb7-bbbe-4a07-b153-93a8463cf744 -->\n\n599 <!-- marginalia, from page 0 (l=0.915,t=0.962,r=0.950,b=0.977), with ID 7bf954be-2db6-417d-ab4f-30f13ce280d6 -->\n\nArticle <!-- marginalia, from page 0 (l=0.061,t=0.025,r=0.128,b=0.045), with ID ed38b625-725f-427d-b2f1-3788ce367051 -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.649,t=0.026,r=0.949,b=0.045), with ID 87d9440c-22f0-421c-8d74-68e732a60484 -->\n\n35. Huang, Y., Wilches, J. & Sun, Y. Robot gaining accurate pouring skills through self-supervised learning and generalization. Robot. Auton. Syst. **136**, 103692 (2021).\n36. Mon-Williams, R., Stouraitis, T. & Vijayakumar, S. A behavioural transformer for effective collaboration between a robot and a non-stationary human. In 2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN) 1150‚Äì1157 (IEEE, 2023).\n37. Belkhale, S., Cui, Y. & Sadigh, D. Data quality in imitation learning. In Advances in Neural Information Processing Systems (NeurIPS) 80375‚Äì80395 (Curran Associates, 2024).\n38. Khazatsky, A. et al. DROID: a large-scale in-the-wild robot manipulation dataset. Robotics: Science and Systems; https://www.roboticsproceedings.org/rss20/p120.pdf (2024).\n39. Acosta, B., Yang, W. & Posa, M. Validating robotics simulators on real-world impacts. IEEE Robot. Autom. Lett. **7**, 6471‚Äì6478 (2022).\n40. Alomar, A. et al. CausalSim: a causal framework for unbiased trace-driven simulation. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23) 1115‚Äì1147 (USENIX Association, 2023).\n41. Choi, H. et al. On the use of simulation in robotics: opportunities, challenges, and suggestions for moving forward. Proc. Natl. Acad. Sci. USA **118**, e190785611 (2021).\n42. Del Aguila Ferrandis, J., Moura, J. & Vijayakumar, S. Nonprehensile planar manipulation through reinforcement learning with multimodal categorical exploration. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 5606‚Äì5613 (IEEE, 2023).\n43. Kirk, R., Zhang, A., Grefenstette, E. & Rockt√§schel, T. A survey of zero-shot generalisation in deep reinforcement learning. J. Artific. Intell. Res. **76**, 201‚Äì264 (2023).\n44. Dai, T. et al. Analysing deep reinforcement learning agents trained with domain randomisation. Neurocomputing **493**, 143‚Äì165 (2022).\n45. Chang, J., Uehara, M., Sreenivas, D., Kidambi, R. & Sun, W. Mitigating covariate shift in imitation learning via offline data with partial coverage. In Advances in Neural Information Processing Systems 965‚Äì979 (Curran Associates, 2021).\n46. Huang, W. et al. Inner monologue: embodied reasoning through planning with language models. In Proc. 6th Conference on Robot Learning 1769‚Äì1782 (PMLR, 2023).\n47. Nair, S., Rajeswaran, A., Kumar, V., Finn, C. & Gupta, A. R3M: a universal visual representation for robot manipulation. In Proc. 6th Conference on Robot Learning Vol. 205, 892‚Äì909 (PMLR, 2022).\n48. Singh, I. et al. ProgPrompt: generating situated robot task plans using large language models. In Proc. IEEE/CVF International Conference on Robotics and Automation (ICRA) 11523‚Äì11530 (IEEE, 2023).\n49. Song, C. H. et al. LLM-Planner: few-shot grounded planning for embodied agents with large language models. In Proc. IEEE/CVF International Conference on Computer Vision (ICCV) 2998‚Äì3009 (IEEE/CVF, 2023).\n50. Vemprala, S. H., Bonatti, R., Bucker, A. & Kapoor, A. ChatGPT for robotics: design principles and model abilities. IEEE Access 12, 55682‚Äì55696 (2024).\n51. Ding, Y., Zhang, X., Paxton, C. & Zhang, S. Task and motion planning with large language models for object rearrangement. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2086‚Äì2092 (IEEE, 2023).\n52. Kwon, M. et al. Toward grounded commonsense reasoning. In Proc. International Conference on Robotics and Automation (ICRA) 5463‚Äì5470 (IEEE, 2024).\n53. Hong, J., Levine, S. & Dragan, A. Learning to influence human behavior with offline reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS) 36094‚Äì36105 (Curran Associates, 2023). <!-- text, from page 0 (l=0.059,t=0.053,r=0.498,b=0.946), with ID 51ae8d76-3ef8-455d-8ebf-22e2085d64e9 -->\n\n54. OpenAI. GPT-4 technical report. Preprint at http://arxiv.org/ abs/2303.08774 (2024).\n55. OpenAI. Custom models program: fine-tuning GPT-4 for specific domains (2023); https://platform.openai.com/docs/guides/ fine-tuning/\n56. Pietsch, M. et al. Haystack: the end-to-end nlp framework for pragmatic builders. GitHub https://github.com/deepset-ai/ haystack (2019).\n57. Weaviate. Verba: the golden RAGretriever. GitHub https://github. com/weaviate/Verba (2023).\n58. Kirillov, A. et al. Segment anything. In Proc. IEEE/CVF International Conference on Computer Vision (ICCV) 4015‚Äì4026 (IEEE, 2023).\n59. Ramesh, A. et al. Zero-shot text-to-image generation. In Proc. 38th International Conference on Machine Learning 8281‚Äì8831 (PMLR, 2021).\n60. Zeng, A. et al. Socratic models: composing zero-shot multimodal reasoning with language. In Proc. International Conference on Learning Representations (ICLR, 2023).\n61. Cui, Y. et al. No, to the right: online language corrections for robotic manipulation via shared autonomy. In Proc. 2023 ACM/ IEEE International Conference on Human-Robot Interaction, HRI ‚Äô23 93‚Äì101 (Association for Computing Machinery, 2023).\n62. Bengio, Y. et al. Managing extreme AI risks amid rapid progress. Science 384, 842‚Äì845 (2024).\n63. Li, G., Jampani, V., Sun, D. & Sevilla-Lara, L. Locate: localize and transfer object parts for weakly supervised affordance grounding. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 10922‚Äì10931 (IEEE, 2023).\n64. Li, G., Sun, D., Sevilla-Lara, L. & Jampani, V. One-shot open set affordance learning with foundation models. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 3086‚Äì3096 (IEEE, 2024).\n65. Liang, J. et al. Code as policies: language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA) 9493‚Äì9500 (IEEE, 2023).\n66. Hong, S. & Kim, H. An integrated GPU power and performance model. In Proc. 37th Annual International Symposium on Computer Architecture 280‚Äì289 (Association for Computing Machinery, 2010).\n67. Kinova Robotics. Kinova Gen3 Ultra-Lightweight Robotic Arm User Guide (2023); https://assets.iqr-robornt.com/wp-content/ uploads/2023/08/20230814163651088831.pdf\n68. US Environmental Protection Agency. GHG emission factors hub (2024); https://www.epa.gov/climateleadership/ ghg-emission-factors-hub\n69. Liu, S. et al. Grounding DINO: marrying DINO with grounded pre-training for open-set object detection. In 2024 European Conference on Computer Vision (eds Iandola, S. et al.) Vol. 15105 (Springer, 2023).\n70. ruadhrin/noa.foundation/ELLMER: v1.0.0: Initial Release. Zenodo https://doi.org/10.5281/zenodo.14483539 (2024). <!-- text, from page 0 (l=0.508,t=0.053,r=0.948,b=0.768), with ID 0b1f6d83-bcf4-45f4-9dd7-58655d768e16 -->\n\n**Acknowledgements**  \nThis work was supported by the EPSRC CDT in RAS (EP/L016834/1). We thank S. Vijayakumar for his support and for providing access to resources; L. Martins and the Edinburgh Workshop for their assistance with hardware; and J. Wang, T. Stouraitis, J. Ferrandis and many others for their invaluable support and expertise. <!-- text, from page 0 (l=0.510,t=0.775,r=0.943,b=0.863), with ID 586953c6-9630-4876-854c-debc6757b09e -->\n\nAuthor contributions  \nConceptualization: R.M.-W., G.L. and R.L. Methodology: R.M.-W., G.L., R.L., W.D. and C.G.L. Software: R.M.-W., G.L., R.L. and W.D. Formal analysis: R.M.-W. Investigation: R.M.-W. and W.D. Visualization: R.M.-W. Validation: R.M.-W. and W.D. Writing‚Äîoriginal draft: <!-- text, from page 0 (l=0.510,t=0.871,r=0.945,b=0.945), with ID 300d7da4-aef5-4f1d-8e57-f65a4177b2e4 -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.062,t=0.959,r=0.452,b=0.977), with ID 6a4e850a-0c86-4878-9118-32362eff1746 -->\n\n600 <!-- marginalia, from page 0 (l=0.912,t=0.962,r=0.948,b=0.977), with ID 538a3b37-da48-43a1-9a82-7ba0de4a4743 -->\n\nArticle <!-- marginalia, from page 0 (l=0.061,t=0.025,r=0.128,b=0.045), with ID f295f87e-0849-4b89-9605-9b1ca4bd27f2 -->\n\nhttps://doi.org/10.1038/s42256-025-01005-x <!-- marginalia, from page 0 (l=0.648,t=0.026,r=0.948,b=0.045), with ID 3f0da87b-5802-49ef-9236-387a64dbb828 -->\n\nR.M.-W. Writing‚Äîreview and editing: R.M.-W., G.L., R.L. and C.G.L.\nSupervision: C.G.L. <!-- text, from page 0 (l=0.061,t=0.056,r=0.474,b=0.088), with ID 4f79f0d3-6694-42f8-a76a-4eaa936f27ed -->\n\n**Competing interests**  \nThe authors declare no competing interests. <!-- text, from page 0 (l=0.062,t=0.096,r=0.341,b=0.130), with ID c818f647-cba5-4569-83b1-fe770e7defab -->\n\n## Additional information\n**Supplementary information** The online version contains supplementary material available at https://doi.org/10.1038/s42256-025-01005-x. <!-- text, from page 0 (l=0.061,t=0.138,r=0.498,b=0.184), with ID c2c6ad6e-2ae3-41d3-adb5-cb22796da5fe -->\n\n**Correspondence and requests for materials** should be addressed to Ruaridh Mon-Williams or Gen Li. <!-- text, from page 0 (l=0.062,t=0.194,r=0.488,b=0.225), with ID 9b75625d-2507-4e76-8d63-035c1d15dfd3 -->\n\n**Peer review information** *Nature Machine Intelligence* thanks Matteo Saveriano and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. <!-- text, from page 0 (l=0.062,t=0.235,r=0.495,b=0.279), with ID 1f7686cb-c7a4-4104-9f97-e0239f84e60f -->\n\n**Reprints and permissions information** is available at  \nwww.nature.com/reprints. <!-- text, from page 0 (l=0.062,t=0.290,r=0.389,b=0.319), with ID 843ed970-d88d-4cb6-84f6-ebf317ffc3ad -->\n\n**Publisher‚Äôs note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. <!-- text, from page 0 (l=0.510,t=0.057,r=0.889,b=0.102), with ID b3799e2d-fab1-48ec-b248-b33d72eadd80 -->\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. <!-- text, from page 0 (l=0.509,t=0.113,r=0.942,b=0.292), with ID 0d4b48d5-babd-4bcb-834e-452c81c3f208 -->\n\n¬© The Author(s) 2025 <!-- text, from page 0 (l=0.510,t=0.303,r=0.649,b=0.321), with ID 7e3ce6ff-bb67-4116-a8ef-28b06a52dc22 -->\n\nNature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601 <!-- marginalia, from page 0 (l=0.062,t=0.960,r=0.451,b=0.978), with ID 8929aeeb-7c55-4537-a7ec-05e8be64d31f -->\n\n601 <!-- marginalia, from page 0 (l=0.917,t=0.963,r=0.950,b=0.977), with ID d15b5638-6c58-4b3c-a6df-521396939a56 -->",
  "chunks": [
    {
      "text": "nature machine intelligence",
      "grounding": [
        {
          "box": {
            "l": 0.061800479888916016,
            "t": 0.047606345266103745,
            "r": 0.42043861746788025,
            "b": 0.07741818577051163
          },
          "page": 0
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "b44749b6-92c2-4e9f-bc79-aabb1a3de6ba"
    },
    {
      "text": "Summary : This is a logo representing the concept of open access, commonly used in academic publishing to indicate that content is freely available to the public.\n\nlogo:  \nDesign Elements :  \n  ‚Ä¢ Orange open padlock symbol, with a circular body and a semicircular shackle open at the top left.  \n  ‚Ä¢ The padlock body contains a smaller filled orange circle at its center.  \n  ‚Ä¢ No text, company name, or tagline is present.  \n  ‚Ä¢ The logo is set against a white background with a thin blue line at the very top edge.\n\nDimensions & Placement :  \n  ‚Ä¢ The logo is vertically oriented, with the shackle at the top and the circular body below.  \n  ‚Ä¢ The blue line is horizontal and positioned at the very top of the image.\n\nAnalysis :  \n  ‚Ä¢ The open padlock visually communicates unrestricted access, aligning with the principles of open access publishing.  \n  ‚Ä¢ The use of orange is typical for open access branding, making the symbol easily recognizable in academic contexts.",
      "grounding": [
        {
          "box": {
            "l": 0.9036070108413696,
            "t": 0.048505254089832306,
            "r": 0.9486477375030518,
            "b": 0.08586319535970688
          },
          "page": 0
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "76ebb208-f221-4c7f-a90e-536f844f56d2"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.06031361594796181,
            "t": 0.11447332799434662,
            "r": 0.1278775930404663,
            "b": 0.1334616094827652
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "f06c699d-02e6-4214-afc3-ee8eeb73313d"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6485301852226257,
            "t": 0.11517048627138138,
            "r": 0.9490883946418762,
            "b": 0.13530172407627106
          },
          "page": 0
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "971555ca-6b04-46ad-abb7-a147c43040df"
    },
    {
      "text": "Embodied large language models enable\nrobots to complete complex tasks in\nunpredictable environments",
      "grounding": [
        {
          "box": {
            "l": 0.06033354997634888,
            "t": 0.14005573093891144,
            "r": 0.8626804947853088,
            "b": 0.2507285475730896
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "33f62124-a73a-413e-aedd-ce6e8bb63bb9"
    },
    {
      "text": "Received: 22 June 2024\n\nAccepted: 31 January 2025\n\nPublished online: 19 March 2025\n\nüü† Check for updates",
      "grounding": [
        {
          "box": {
            "l": 0.06066316366195679,
            "t": 0.2999327480792999,
            "r": 0.3492421805858612,
            "b": 0.4020289480686188
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "22629fba-48e0-4a43-9b60-8e8d5043b5f6"
    },
    {
      "text": "Ruaridh Mon-Williams ![inline figure: circled \"i\" icon](ORCID)¬π,¬≤,¬≥ ‚úâ, Gen Li ![inline figure: circled \"i\" icon](ORCID)¬π ‚úâ, Ran Long ![inline figure: circled \"i\" icon](ORCID)¬π, Wenqian Du ![inline figure: circled \"i\" icon](ORCID)¬π,‚Å¥ & Christopher G. Lucas¬π",
      "grounding": [
        {
          "box": {
            "l": 0.3581998348236084,
            "t": 0.2992153763771057,
            "r": 0.9189784526824951,
            "b": 0.3385988473892212
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "157c6f2a-6e4e-43f5-81cc-e7758d7d4da0"
    },
    {
      "text": "Completing complex tasks in unpredictable settings challenges robotic\nsystems, requiring a step change in machine intelligence. Sensorimotor\nabilities are considered integral to human intelligence. Thus, biologically\ninspired machine intelligence might usefully combine artificial intelligence\nwith robotic sensorimotor capabilities. Here we report an embodied\nlarge-language-model-enabled robot (ELLMER) framework, utilizing GPT-4\nand a retrieval-augmented generation infrastructure, to enable robots to\ncomplete long-horizon tasks in unpredictable settings. The method extracts\ncontextually relevant examples from a knowledge base, producing action\nplans that incorporate force and visual feedback and enabling adaptation\nto changing conditions. We tested ELLMER on a robot tasked with coffee\nmaking and plate decoration; these tasks consist of a sequence of sub-tasks\nfrom drawer opening to pouring, each benefiting from distinct feedback\ntypes and methods. We show that the ELLMER framework allows the robot\nto complete the tasks. This demonstration marks progress towards scalable,\nefficient and ‚Äòintelligent robots‚Äô able to complete complex tasks in uncertain\nenvironments.",
      "grounding": [
        {
          "box": {
            "l": 0.3597124218940735,
            "t": 0.3616790175437927,
            "r": 0.9452138543128967,
            "b": 0.6644484400749207
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "6adcde0f-defb-4e03-ba5b-b6c3737ff06a"
    },
    {
      "text": "If Deep Blue (the first computer to win a chess match against a reigning\nworld champion) was truly intelligent, then should it not be able to\nmove its own pieces when playing chess? Intelligence is a multifaceted\nconstruct and, thus, difficult to define. Consequently, human intel-\nligence and its assessment is a controversial topic¬π. However, there is\na growing consensus that human intelligence is best understood as\n‚Äòembodied cognition‚Äô, where attention, language, learning, memory\nand perception are not abstract cognitive processes constrained to\nthe brain but intrinsically linked with how the body interacts with its\nsurrounding environment¬≤Àí¬≥. Indeed, there is growing evidence that\nhuman intelligence has its ontological and phylogenetic foundations\nin sensorimotor processes‚Å¥.",
      "grounding": [
        {
          "box": {
            "l": 0.06071312725543976,
            "t": 0.6830369830131531,
            "r": 0.49980562925338745,
            "b": 0.8474946618080139
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "7c0053ce-9ccd-40ff-a969-fad6940e7603"
    },
    {
      "text": "Embodied cognition has theoretical implications for ‚Äòmachine\nintelligence‚Äô as it suggests that machines will be unable to demon-\nstrate some aspects of intelligence if ‚Äòcognitive‚Äô processes are not",
      "grounding": [
        {
          "box": {
            "l": 0.061337605118751526,
            "t": 0.8475569486618042,
            "r": 0.49902528524398804,
            "b": 0.8902173042297363
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "8bfbed06-74a7-4cd4-93fd-b913cd506bda"
    },
    {
      "text": "embedded in a robotic device. This is a conjecture that is still to be\ntested, but ‚Äòintelligent robots‚Äô provide an effective way of exploring\nvarious hypotheses concerning human intelligence and advancing the\nfield of machine intelligence. More practically, effective human‚Äìrobot\ncollaboration will ultimately require robots to at least approximate\n‚Äòhuman-like‚Äô capabilities. Thus, a reasonable expectation of future\n‚Äòintelligent machines‚Äô is that they will have the potential to perform\nabstract cognitive computations as they skilfully interact with objects\nand humans within their environment‚Åµ.",
      "grounding": [
        {
          "box": {
            "l": 0.5105668306350708,
            "t": 0.6841537356376648,
            "r": 0.9470776319503784,
            "b": 0.8065800070762634
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "06024d76-3353-4a02-957f-f67eea30774e"
    },
    {
      "text": "So far, parallel streams of activity have advanced: (1) the sensori-\nmotor abilities of robots and (2) artificial intelligence‚Å∂. We set out to test\nthe hypothesis that these approaches can now be combined to create\na step change in the ability of robots to show human-like intelligence.\nWe further hypothesized that integrating (1) and (2) would allow robots\nto undertake the type of complex tasks that are practically useful in a",
      "grounding": [
        {
          "box": {
            "l": 0.5120788812637329,
            "t": 0.8076816201210022,
            "r": 0.9464290142059326,
            "b": 0.8902669548988342
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "3abcc5f4-f905-45e8-a6bc-cc278d5c5abd"
    },
    {
      "text": "¬πUniversity of Edinburgh, Edinburgh, UK. ¬≤Massachusetts Institute of Technology, Boston, MA, USA. ¬≥Princeton University, Princeton, NJ, USA. ‚Å¥Alan Turing Institute, London, UK. ‚úâe-mail: ruaridh.mw@ed.ac.uk; li.gen@ed.ac.uk",
      "grounding": [
        {
          "box": {
            "l": 0.06340152025222778,
            "t": 0.913955807685852,
            "r": 0.867966890335083,
            "b": 0.945744514465332
          },
          "page": 0
        }
      ],
      "chunk_type": "text",
      "chunk_id": "ccf5713e-1dce-4a7b-8f26-9af81155fe81"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.06225317716598511,
            "t": 0.9591482281684875,
            "r": 0.4516703486442566,
            "b": 0.9783119559288025
          },
          "page": 0
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "962b8177-6fd0-4f37-a1aa-2a817399dbe1"
    },
    {
      "text": "592",
      "grounding": [
        {
          "box": {
            "l": 0.9139404892921448,
            "t": 0.9617151021957397,
            "r": 0.9496909976005554,
            "b": 0.9774342775344849
          },
          "page": 0
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "0e0e0b53-8b5c-4571-b478-f3a42316e255"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.061956826597452164,
            "t": 0.025939390063285828,
            "r": 0.12595196068286896,
            "b": 0.04476883262395859
          },
          "page": 1
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "582562ff-abb7-4eca-823f-481c8b2aeedd"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6499911546707153,
            "t": 0.02617644891142845,
            "r": 0.947656512260437,
            "b": 0.04469442740082741
          },
          "page": 1
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "a6698873-b0f5-4c34-8c93-304909f228b6"
    },
    {
      "text": "wide range of settings but currently outwith the capability of robotic\nsystems. Consider a scenario in which someone returns home feeling\nfatigued and thirsty. A robot with a sophisticated manipulation system\nis situated in the homeowner‚Äôs kitchen and is instructed to prepare\na drink. The robot decides that a reinvigorating cup of coffee needs\nto be made and handed to their carbon companion. This task‚Äî\nstraightforward for humans‚Äîencompasses a series of challenges that\ntest the limits of current robotic capabilities7‚Äì11. First, the robot must\ninterpret the information it receives and analyse its surroundings.\nNext, it may need to search the environment to locate a mug. This could\ninvolve opening drawers with unspecified opening mechanisms. Then,\nthe robot must measure and mix the precise ratio of water to coffee. This\nrequires fine-grained force control and adaptation to uncertainty if, for\nexample, the human moves the location of the mug unexpectedly9,12.\nThis scenario is a canonical example of the multifaceted nature of\ncomplex tasks in dynamic environments. Robotic systems have tradi-\ntionally struggled with these tasks because they have been unable to\nfollow high-level commands, have relied on preprogrammed responses\nand lack the flexibility to adapt seamlessly to perturbations13,14.",
      "grounding": [
        {
          "box": {
            "l": 0.06186835467815399,
            "t": 0.057138487696647644,
            "r": 0.4993159770965576,
            "b": 0.31736916303634644
          },
          "page": 1
        }
      ],
      "chunk_type": "text",
      "chunk_id": "7679520c-1039-4884-bb5b-fa271fd0207f"
    },
    {
      "text": "Reinforcement learning and imitation learning have demonstrated the effectiveness of interaction and demonstration in teaching robots to perform complex tasks. These approaches are promising¬π‚Åµ, but often struggle with adaptation to novel tasks and coping with diverse scenarios. Imitation learning also faces challenges when a robot needs to adapt to new contexts¬π‚Å∂‚Åª¬≤¬≥. Nature-inspired machine intelligence provides a potential solution to these challenges. The sophistication of human manipulation is due, in part, to the type of cognitive processes that are captured artificially by large language models (LLMs)¬≤‚Å¥‚Åª¬≤‚Å∂. LLMs offer a way to process complex instructions and adapt actions accordingly because of their advanced contextual understanding and generalization abilities¬≤‚Å∑,¬≤‚Å∏.",
      "grounding": [
        {
          "box": {
            "l": 0.06314639747142792,
            "t": 0.3177289664745331,
            "r": 0.4986231327056885,
            "b": 0.4808153808116913
          },
          "page": 1
        }
      ],
      "chunk_type": "text",
      "chunk_id": "97d3c7b6-2287-49ca-b54b-d53a6ad59a33"
    },
    {
      "text": "A large body of recent research has used LLMs for short-horizon tasks15,27,29. For instance, VoxPoser utilizes LLMs to perform a variety of everyday manipulation tasks15. Similarly, Robotics Transformer (RT-2) leverages large-scale web and robotic learning data, enabling robots to perform tasks beyond the training scenario with remarkable adaptability29. Hierarchical diffusion policy introduces a model structure to generate context-aware motion trajectories, which enhances task-specific motions from high-level LLM decision inputs30. However, challenges remain in effectively integrating LLMs into robotic manipulation. These challenges include complex prompting requirements, a lack of real-time interacting feedback, a dearth of LLM-driven work exploiting the use of force feedback and inefficient pipelines that hinder the seamless execution of tasks15,31. Moreover, current approaches have neglected the application of retrieval-augmented generation (RAG)32 in robotics despite RAG‚Äôs potential to continually update and refine robot knowledge with relevant and accurate examples (and increase the knowledge base without impacting performance). Robot capacity is also limited because force and visual feedback are typically not integrated in robot sensorimotor control15,33. This integration is crucial in scenarios such as pouring water into a moving cup, where vision is necessary to track the cup and force feedback is needed for pouring the desired amount of water when vision is occluded16,34,35. Thus, there is a need for an innovative approach in robot manipulation that combines the best of artificial ‚Äòcognition‚Äô with integrated ‚Äòsensorimotor‚Äô visual and force feedback capabilities to effectively execute actions in the face of uncertainty. Supplementary Section 1 provides more background on state-of-the-art approaches and their current limitations36‚Äì53.",
      "grounding": [
        {
          "box": {
            "l": 0.06325814127922058,
            "t": 0.4822924733161926,
            "r": 0.49766966700553894,
            "b": 0.8463885188102722
          },
          "page": 1
        }
      ],
      "chunk_type": "text",
      "chunk_id": "0523fc33-c640-47ab-ae30-4a37a543f34f"
    },
    {
      "text": "Embodied LLM-enabled robot (ELLMER) is a framework that\nintegrates approaches in artificial intelligence and sensorimotor\ncontrol to create a step change in robotic capabilities. Its usefulness\narises from its combined use of vision and force for sensorimotor\nfeedback control uniquely coupled with the cognitive capabilities\nafforded through an integrated LLM combined with RAG and a curated\nknowledge base. We hypothesized that ELLMER would allow a robot",
      "grounding": [
        {
          "box": {
            "l": 0.06454907357692719,
            "t": 0.84808349609375,
            "r": 0.49766039848327637,
            "b": 0.9432096481323242
          },
          "page": 1
        }
      ],
      "chunk_type": "text",
      "chunk_id": "081062ec-e906-45da-8a98-cfbf1ae1aadb"
    },
    {
      "text": "Summary : This figure presents a schematic overview of a hierarchical robot control system that integrates a large language model (LLM, specifically GPT-4) with a robot controller, vision system, and feedback loops for manipulation tasks. The diagram illustrates both high-level (task decomposition, code generation) and low-level (robot control, feedback) system components and their interactions.\n\nflowchart:\n# Nodes :\n  ‚Ä¢ User Query (rectangle): Represents user input in natural language.\n  ‚Ä¢ Transformer LLM (rounded rectangle): Processes user queries, integrates environment image and code database, decomposes tasks, retrieves and adapts code, and outputs Python code.\n  ‚Ä¢ Code (rectangle with code icon): Symbolizes generated Python code.\n  ‚Ä¢ Database (cylinder): Stores code examples and functions.\n  ‚Ä¢ Environment Image (camera icon): Azure Kinect depth camera providing visual input.\n  ‚Ä¢ Robot Controller (circle labeled \"A\"): Receives code and sends control signals to the robot.\n  ‚Ä¢ Robot Arm (robotic arm illustration): Executes actions based on controller commands.\n  ‚Ä¢ Force/Torque Sensor (rectangle): Provides force/torque feedback.\n  ‚Ä¢ Vision System (camera): Provides visual feedback for object recognition and pose estimation.\n  ‚Ä¢ Feedback Loops (arrows): Visual, force, and torque feedback paths to the controller.\n  ‚Ä¢ Trajectory Plot (3D axes with colored lines): Illustrates robot end-effector trajectories.\n\n# Connectors :\n  ‚Ä¢ User Query ‚Üí Transformer LLM (dashed arrow): User input is sent to the LLM.\n  ‚Ä¢ Transformer LLM ‚Üî Environment Image (dashed arrow): LLM receives image context.\n  ‚Ä¢ Transformer LLM ‚Üî Database (dashed arrow): LLM retrieves code examples.\n  ‚Ä¢ Transformer LLM ‚Üí Code (dashed arrow): LLM outputs Python code.\n  ‚Ä¢ Code ‚Üí Robot Controller (solid arrow): Code is sent to the controller.\n  ‚Ä¢ Robot Controller ‚Üí Robot Arm (solid arrow): Controller sends control signals (Œª).\n  ‚Ä¢ Robot Arm ‚Üí Vision System (solid arrow): Robot state is observed visually.\n  ‚Ä¢ Vision System ‚Üí Robot Controller (solid arrow): Visual feedback (s_visual) is sent to the controller.\n  ‚Ä¢ Robot Arm ‚Üí Force/Torque Sensor (solid arrow): Physical interaction is measured.\n  ‚Ä¢ Force/Torque Sensor ‚Üí Robot Controller (solid arrow): Force/torque feedback (s_force, s_torque) is sent to the controller.\n  ‚Ä¢ Robot Controller ‚Üí Trajectory Plot (solid arrow): Controller generates trajectories.\n  ‚Ä¢ Feedback Loops: Visual, force, and torque feedback are shown as separate labeled arrows.\n\n# Layout :\n  ‚Ä¢ The diagram is organized in two horizontal layers: the upper layer (above the dashed line) for high-level LLM-based planning and code generation, and the lower layer (below the dashed line) for low-level robot control and feedback.\n  ‚Ä¢ Arrows indicate the flow of information and control, with feedback loops closing from sensors back to the controller.\n\n# Analysis :\n  ‚Ä¢ The figure demonstrates a closed-loop, hierarchical robot control architecture where a transformer LLM decomposes user queries into executable code, leveraging both environmental context and a code database.\n  ‚Ä¢ The system integrates multimodal feedback (vision, force, torque) to enable precise manipulation, such as grasping and moving objects.\n  ‚Ä¢ The use of feedback loops (visual, force, torque) allows the robot to adapt to disturbances and update its actions in real time, as illustrated by the trajectory plot and feedback arrows.\n  ‚Ä¢ The architecture supports modularity and adaptability, with clear separation between high-level planning and low-level execution.",
      "grounding": [
        {
          "box": {
            "l": 0.5093185901641846,
            "t": 0.055111587047576904,
            "r": 0.9433687925338745,
            "b": 0.5601294636726379
          },
          "page": 1
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "5f8fcb90-e89c-447e-a438-7e61c94a11f4"
    },
    {
      "text": "to make a cup of coffee for a human. We tested this hypothesis using\na seven-degrees-of-freedom Kinova robotic arm to execute the com-\nplex, force-intensive task in an uncertain environment, leveraging\nintegrated force and vision feedback. The overall system diagram is\npresented in Fig. 1.",
      "grounding": [
        {
          "box": {
            "l": 0.511252224445343,
            "t": 0.5891680717468262,
            "r": 0.9463123679161072,
            "b": 0.659378170967102
          },
          "page": 1
        }
      ],
      "chunk_type": "text",
      "chunk_id": "bae2fc79-170a-4ca0-a62a-57727e010cfe"
    },
    {
      "text": "Results\nLanguage integration for abstract reasoning\nWe found that our approach allowed the robot to respond to an abstract\nhigh-order verbal prompt (‚ÄòI‚Äôm tired, with friends due for cake soon.\nCan you make me a hot beverage, and decorate the plate with a random\nanimal of your choice‚Äô) and complete the desired task of making a hot\ndrink and drawing a shape on a plate. The video demonstrating this\ninteraction is provided in Supplementary Video 1. The robot responded\nto the prompt with the following sound output: ‚ÄòI‚Äôm setting things up to\nmake a hot beverage, and also to decorate the plate with a random ani-\nmal drawing. First, I‚Äôll find a mug then scoop coffee and pour hot water.\nAfter that I‚Äôll take an item, likely a pen, to draw a random animal on the\nplate. It sounds like a fun and cosy time with cake soon.‚Äô. In particular, the\nrobot chose to make a cup of coffee when it responded to the request\nfor a hot beverage for a tired human, and was able to decompose the\nhigher-order behaviour into a series of sub-tasks.",
      "grounding": [
        {
          "box": {
            "l": 0.5116540789604187,
            "t": 0.6699681282043457,
            "r": 0.9457442164421082,
            "b": 0.8876923322677612
          },
          "page": 1
        }
      ],
      "chunk_type": "text",
      "chunk_id": "839c9c66-23a4-4ff9-9fd6-53a47a2a8661"
    },
    {
      "text": "The integration of GPT-4 (ref. 54) was found to equip the robot with the desired capacity for abstract reasoning. GPT-4 is a language model that enables a robot to process user queries and environmental data to break down tasks into actionable steps. Our system was able",
      "grounding": [
        {
          "box": {
            "l": 0.5125167965888977,
            "t": 0.8893805146217346,
            "r": 0.9454556107521057,
            "b": 0.9431741833686829
          },
          "page": 1
        }
      ],
      "chunk_type": "text",
      "chunk_id": "98d8e7ec-4e8c-4bc8-b92d-d13caedc1c1e"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.06439310312271118,
            "t": 0.9601584076881409,
            "r": 0.44909733533859253,
            "b": 0.9760997891426086
          },
          "page": 1
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "ee22844d-8654-454b-a65b-59b88176b6bd"
    },
    {
      "text": "593",
      "grounding": [
        {
          "box": {
            "l": 0.9168636202812195,
            "t": 0.9628847241401672,
            "r": 0.9477977156639099,
            "b": 0.9764016270637512
          },
          "page": 1
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "1d137b88-4f40-4f64-a0e6-3849a017b3b4"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.06123225763440132,
            "t": 0.025610076263546944,
            "r": 0.1270677000284195,
            "b": 0.04536289721727371
          },
          "page": 2
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "1f3a5a61-1665-4053-98d5-72a85cc3e14f"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6488861441612244,
            "t": 0.02572854980826378,
            "r": 0.9481928944587708,
            "b": 0.045661721378564835
          },
          "page": 2
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "ba3d5233-779c-4349-98c6-e56320ffd7c2"
    },
    {
      "text": "Summary : This photograph shows a robotic arm in the process of opening a cabinet door, demonstrating a manipulation task in a laboratory or testing environment.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm (labelled \"KINOVA\") equipped with a gripper, actively pulling open a white cabinet door with a red handle.\n  ‚Ä¢ The setting appears to be an indoor laboratory or workspace, with a neutral background and three small boxes (possibly tea or spice containers) placed on top of the cabinet.\n  ‚Ä¢ The perspective is a close-up, side view, focusing on the interaction between the robot and the door.\n\nTechnical Details :\n  ‚Ä¢ No visible scale bar or measurement units.\n  ‚Ä¢ The robotic arm is fitted with a soft, articulated gripper.\n  ‚Ä¢ The cabinet door is white with a red rectangular handle.\n  ‚Ä¢ No on-image UI elements or digital overlays.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic arm is positioned to the right, reaching leftward to grasp and open the door.\n  ‚Ä¢ The door is partially open, with the gripper holding the handle.\n  ‚Ä¢ The three boxes are aligned on top of the cabinet, in the background.\n\nAnalysis :\n  ‚Ä¢ The image visually documents the robot's ability to perform a common household or laboratory manipulation task (door opening), highlighting dexterity and precision in grasping and pulling actions.\n  ‚Ä¢ The presence of objects on the cabinet suggests a realistic, cluttered environment for testing robotic manipulation.",
      "grounding": [
        {
          "box": {
            "l": 0.1936667412519455,
            "t": 0.06524978578090668,
            "r": 0.5087162256240845,
            "b": 0.2237856537103653
          },
          "page": 2
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "679656b0-9f35-4050-8cde-534921354150"
    },
    {
      "text": "Summary : This photograph shows a robotic arm performing a pick-and-place task, grasping a white cup in a kitchen-like environment.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm with a black gripper holding a white cup.\n  ‚Ä¢ The setting appears to be a kitchen or laboratory workspace, with a metallic container and a red wall-mounted fixture in the background.\n  ‚Ä¢ The perspective is from the side, focusing on the interaction between the robot and the cup.\n  ‚Ä¢ Lighting is even and neutral, with a colour palette dominated by white, grey, and black tones.\n\nTechnical Details :\n  ‚Ä¢ No scale bar or magnification is present.\n  ‚Ä¢ No visible on-image UI elements or text, except for partial text on the cup (\"IRE\").\n  ‚Ä¢ The robot arm has a white and grey casing with visible cabling.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic gripper is in the foreground, holding the cup near a flat surface.\n  ‚Ä¢ The cup is positioned close to a metallic container and a red fixture on the wall.\n  ‚Ä¢ The background includes a white cabinet and a metallic object with a knob.\n\nAnalysis :\n  ‚Ä¢ The image visually documents the robot's ability to grasp and manipulate objects, specifically demonstrating a pick-and-place operation with a cup in a realistic environment.",
      "grounding": [
        {
          "box": {
            "l": 0.5105841755867004,
            "t": 0.06414442509412766,
            "r": 0.8179982304573059,
            "b": 0.22546687722206116
          },
          "page": 2
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "2552ed05-48eb-4c95-8a9e-8d20496d7266"
    },
    {
      "text": "Summary : This photograph shows a robotic arm performing a scooping action with a spoon, transferring a granular substance from an open jar, in a laboratory or kitchen-like environment.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm equipped with a gripper holding a metal spoon.\n  ‚Ä¢ The spoon is being used to scoop a brown granular substance (possibly coffee or soil) from an open glass jar.\n  ‚Ä¢ The setting includes a blue cylindrical container with utensils (possibly straws or stirrers) on the left, and a white tissue box on the right.\n  ‚Ä¢ The background features a white cabinet with red handles, suggesting a laboratory or organized workspace.\n  ‚Ä¢ The lighting is even and neutral, with no dramatic shadows or highlights.\n\nTechnical Details :\n  ‚Ä¢ No visible scale bar or measurement indicators.\n  ‚Ä¢ No on-image text or UI elements except for the label \"Scooping\" below the image and a panel label \"c\" in the upper left corner.\n  ‚Ä¢ The robotic arm appears to be a multi-jointed, industrial or research-grade manipulator.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic arm enters from the right, with the spoon extending toward the center of the image.\n  ‚Ä¢ The jar is in the foreground, slightly left of center, with its lid removed.\n  ‚Ä¢ The blue utensil holder is to the left of the jar, and the tissue box is to the right.\n  ‚Ä¢ The composition centers the action of scooping, with supporting objects arranged around the main activity.\n\nAnalysis :\n  ‚Ä¢ The image visually documents the robotic arm's ability to perform a fine-motor task (scooping) in a realistic, cluttered environment, highlighting dexterity and precision in object manipulation.",
      "grounding": [
        {
          "box": {
            "l": 0.19488663971424103,
            "t": 0.22650101780891418,
            "r": 0.5047314763069153,
            "b": 0.3815685212612152
          },
          "page": 2
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "4d8f528d-0b97-41c9-abec-de0d23e112b0"
    },
    {
      "text": "Summary : This photograph shows a robotic arm performing the task of emptying a spoon into a mug, likely as part of an automated kitchen or service robot demonstration.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm holding a spoon over a white mug labeled \"inform\".\n  ‚Ä¢ The spoon appears to be in the process of emptying its contents into the mug.\n  ‚Ä¢ The setting is a kitchen or laboratory environment, with a black electric kettle, a metal French press, and a box of tea visible nearby.\n  ‚Ä¢ The perspective is from the side, focusing on the interaction between the robot and the mug.\n  ‚Ä¢ Lighting is even and neutral, with no dramatic shadows or highlights.\n  ‚Ä¢ The color palette is mostly neutral (white, black, metallic, and some green from the tea box).\n\nTechnical Details :\n  ‚Ä¢ No scale bar or magnification is present.\n  ‚Ä¢ On-image text includes \"inform\" on the mug and \"Emptying\" as a caption below the image.\n  ‚Ä¢ The robotic arm is equipped with a gripper designed to hold utensils.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic arm is positioned to the right, extending leftward toward the mug.\n  ‚Ä¢ The mug is placed on a flat surface in the foreground, with the kettle and French press behind it.\n  ‚Ä¢ The spoon is angled downward, directly above the mug's opening.\n\nAnalysis :\n  ‚Ä¢ The image visually demonstrates the robot's ability to perform a precise kitchen task‚Äîemptying a spoon into a mug‚Äîsuggesting progress in robotic manipulation and automation in domestic settings.\n  ‚Ä¢ The composition highlights the interaction between the robot and common kitchen objects, emphasizing practical application.",
      "grounding": [
        {
          "box": {
            "l": 0.510348916053772,
            "t": 0.2283039391040802,
            "r": 0.8194626569747925,
            "b": 0.38326212763786316
          },
          "page": 2
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "5d1e0e4c-c370-4771-99fa-bbd127327374"
    },
    {
      "text": "Summary : This photograph shows a robotic arm pouring hot water from a black kettle into a white mug held by a human hand, illustrating a collaborative human-robot interaction in a kitchen setting.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject: A robotic arm is pouring water from a black kettle into a white mug.\n  ‚Ä¢ Setting: Indoor kitchen or break room environment, with tiled wall and tea supplies (Yorkshire Tea box, other containers) visible in the background.\n  ‚Ä¢ Perspective: Side view, focused on the pouring action.\n  ‚Ä¢ Lighting: Even, indoor lighting; no harsh shadows.\n  ‚Ä¢ Colour palette: Neutral tones (white, grey, black) with some red and green accents.\n\nTechnical Details :\n  ‚Ä¢ No scale bar or magnification indicated.\n  ‚Ä¢ On-image text: The mug has a logo and the word \"inform\" printed on it.\n  ‚Ä¢ No visible UI elements or digital overlays.\n\nSpatial Relationships :\n  ‚Ä¢ Foreground: Robotic arm, kettle, mug, and human hand.\n  ‚Ä¢ Background: Tiled wall with red switches, tea boxes, and other kitchen items.\n  ‚Ä¢ The pouring action is centered, with the mug held steady by a human hand on the left and the robotic arm on the right.\n\nAnalysis :\n  ‚Ä¢ The image demonstrates a precise, collaborative task where a robot assists a human by pouring water, highlighting advancements in human-robot interaction and dexterous manipulation in everyday environments.",
      "grounding": [
        {
          "box": {
            "l": 0.18821822106838226,
            "t": 0.38061174750328064,
            "r": 0.506872296333313,
            "b": 0.5382779240608215
          },
          "page": 2
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "cc2eedd3-fdc3-4400-a6d3-977406107c9e"
    },
    {
      "text": "Summary : This photograph shows a robotic arm performing a drawing task on a white plate in a kitchen-like environment, with various household items visible in the background.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a robotic arm equipped with a gripper holding a pen or marker, positioned over a white plate.\n  ‚Ä¢ The robot appears to be drawing or writing on the plate.\n  ‚Ä¢ The setting resembles a kitchen or laboratory, with a white countertop and several household items (e.g., a box of Twinings tea, a wrapped package, a plate with a cookie or biscuit, a glass jar, and a teapot) visible in the background.\n  ‚Ä¢ The perspective is at table height, focusing on the robot's end effector and the plate.\n  ‚Ä¢ Lighting is even and natural, with neutral colour tones.\n\nTechnical Details :\n  ‚Ä¢ No scale bar or explicit measurement references are visible.\n  ‚Ä¢ No on-image UI elements or text overlays, except for the caption \"Drawing\" below the image.\n\nSpatial Relationships :\n  ‚Ä¢ The robotic arm is on the right side of the image, extending toward the center.\n  ‚Ä¢ The plate being drawn on is in the foreground, slightly left of center.\n  ‚Ä¢ Background items are arranged along the back edge of the countertop, providing context but not obstructing the main subject.\n\nAnalysis :\n  ‚Ä¢ The image visually documents a robot performing a fine-motor task (drawing), likely as part of a demonstration of robotic dexterity or kitchen assistance.\n  ‚Ä¢ The presence of food items and kitchenware suggests the robot may be intended for domestic or culinary applications.",
      "grounding": [
        {
          "box": {
            "l": 0.5094714164733887,
            "t": 0.38683006167411804,
            "r": 0.8217108249664307,
            "b": 0.5416021347045898
          },
          "page": 2
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "9ab71824-4c77-4b96-80a9-9886e3bed3a8"
    },
    {
      "text": "Fig. 2| Kinova robot in action. a‚Äìf, Action shots of the Kinova Gen3 robot preparing coffee (a‚Äìe) and decorating a plate (f).",
      "grounding": [
        {
          "box": {
            "l": 0.061079323291778564,
            "t": 0.5415377020835876,
            "r": 0.7159148454666138,
            "b": 0.5586047768592834
          },
          "page": 2
        }
      ],
      "chunk_type": "text",
      "chunk_id": "b4b402d7-886a-499e-8eea-7b9627711662"
    },
    {
      "text": "to generate code and execute actions with force and vision feedback,\neffectively providing the robot with a form of intelligence. Our meth-\nodology was successful in creating a custom GPT-4 (refs. 54,55) with a\ncomprehensive database of flexible motion examples. The database\nsuccessfully incorporated pouring, scooping, drawing, handovers,\npick and place, and opening doors.",
      "grounding": [
        {
          "box": {
            "l": 0.060713157057762146,
            "t": 0.5881577730178833,
            "r": 0.500377893447876,
            "b": 0.6709060668945312
          },
          "page": 2
        }
      ],
      "chunk_type": "text",
      "chunk_id": "a3e9000d-75e9-46bc-83cf-afb890cca19d"
    },
    {
      "text": "We found that the robot could identify and extract relevant examples for the downstream task using RAG. We explored various approaches to determine how intelligent machines could make the best use of RAG via our framework. These approaches included customizable open-source methods, such as Haystack56 and Vebra57, as well as proprietary technologies such as Azure Cloud AI. We found that all of these approaches were viable. In our experiment, we chose the simplest method: logically organizing our curated knowledge base in a markdown file and uploading it to the custom GPT API via the ‚ÄòKnowledge‚Äô feature in the GPT‚Äôs platform. This allowed the platform to automatically handle the retrieval processes and select between semantic search (returning relevant text chunks) or document review (providing complete documents or sections from larger texts). We chose this solution as it provided a state-of-the-art embedder and model, gave ease of use and was able to consistently produce good performance in our task. However, our framework allows the incorporation of a range of RAG techniques and ensures that the ‚Äòintelligent robot‚Äô is able to efficiently complete complex tasks. The curated knowledge base, combined with RAG, allowed the language model to access a large selection of low- and high-order functions, each with",
      "grounding": [
        {
          "box": {
            "l": 0.0617808997631073,
            "t": 0.6718475818634033,
            "r": 0.4995257556438446,
            "b": 0.9439400434494019
          },
          "page": 2
        }
      ],
      "chunk_type": "text",
      "chunk_id": "0ab3ba03-edd0-4c41-ad99-f612eef6526e"
    },
    {
      "text": "known uncertainties. Our tests showed that this capability enabled\nthe robot to handle numerous scenarios effectively.",
      "grounding": [
        {
          "box": {
            "l": 0.5102158784866333,
            "t": 0.5886340141296387,
            "r": 0.947319746017456,
            "b": 0.6184278726577759
          },
          "page": 2
        }
      ],
      "chunk_type": "text",
      "chunk_id": "f63bce43-5c06-45cc-a48c-fc49dcd0dad6"
    },
    {
      "text": "Completing a complex task  \nThe robot was found to skilfully execute the high-level task specified by the user and was able to access a comprehensive motion primitive database. The database included a variety of flexible examples of specific motions and these were successfully carried out by the robotic arm (Fig. 2). Included in the database were examples of pouring liquids; scooping powders; opening doors with unknown mechanisms; picking up and placing objects; drawing any requested shape; conducting handovers; and moving in various directions, orientations or relative to specified objects. The robot was able to replicate and adapt the motions needed to execute the complex tasks requested by the user. The system enabled the robot to dynamically adjust to environmental variables and uncertainties. This enhanced the robot‚Äôs effectiveness in unpredictable conditions, and improved its flexibility and adaptability in the real-world setting.",
      "grounding": [
        {
          "box": {
            "l": 0.5096540451049805,
            "t": 0.6291449666023254,
            "r": 0.9484422206878662,
            "b": 0.8352257609367371
          },
          "page": 2
        }
      ],
      "chunk_type": "text",
      "chunk_id": "88b66330-4499-4fd0-8160-47557de45d0a"
    },
    {
      "text": "Zero-shot pose detection  \nWe found that an Azure Kinect DK Depth Camera, set to a resolution of 640 √ó 576 px¬≤ with a sample rate of 30 fps for depth sensing, was able to provide sufficient visual input for our method. We achieved calibration using a 14-cm AprilTag, and found that this allowed alignment between the camera and the robot‚Äôs base to an accuracy of less than 10‚Åª‚Å∂. This setup enabled accurate object position detection within",
      "grounding": [
        {
          "box": {
            "l": 0.5104747414588928,
            "t": 0.846454381942749,
            "r": 0.9474244713783264,
            "b": 0.9447406530380249
          },
          "page": 2
        }
      ],
      "chunk_type": "text",
      "chunk_id": "c8954f0e-2f7a-41c9-ba4c-6eb9d0e4355b"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.06333714723587036,
            "t": 0.9599771499633789,
            "r": 0.4506111741065979,
            "b": 0.97633957862854
          },
          "page": 2
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "5bc68377-9bca-4693-9b53-17c26e6d0434"
    },
    {
      "text": "594",
      "grounding": [
        {
          "box": {
            "l": 0.9149775505065918,
            "t": 0.9621228575706482,
            "r": 0.9474105834960938,
            "b": 0.9764543175697327
          },
          "page": 2
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "f4dccf66-347a-40fc-b70f-af1fe97a9904"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.06088815629482269,
            "t": 0.02549520879983902,
            "r": 0.12674401700496674,
            "b": 0.04493235796689987
          },
          "page": 3
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "fefc7fb5-2128-465d-ace2-bea2489f9c4e"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6482909917831421,
            "t": 0.025316521525382996,
            "r": 0.9484924077987671,
            "b": 0.04552408307790756
          },
          "page": 3
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "9ee69797-edb3-45c2-ae71-7626722ce601"
    },
    {
      "text": "Summary : This figure shows a robotic vision detection module in action, where a robot arm is presented with a tabletop scene containing various objects. The system uses zero-shot vision detection to identify and localize a hand, a white mug, and a black kettle, each with associated confidence scores, and extracts target poses for robotic grasping.\n\nphoto:\nScene Overview :\n  ‚Ä¢ The main subject is a robotic arm (labeled \"KINOVA\") positioned at the right side of a table, reaching toward objects.\n  ‚Ä¢ The tabletop contains a white mug, a block-shaped black kettle, a hand, a plate, a pastry, a brown box, and other miscellaneous items.\n  ‚Ä¢ The perspective is an overhead and slightly angled view, showing both the robot and the objects it interacts with.\n  ‚Ä¢ Lighting is even, with a neutral color palette dominated by whites and grays.\n\nTechnical Details :\n  ‚Ä¢ On-image UI elements include colored bounding boxes and labels with confidence scores:\n    ‚Äì \"Hand 0.53\" (red box, left side of image)\n    ‚Äì \"White mug 0.70\" (purple box, near the hand)\n    ‚Äì \"Block kettle 0.79\" (red box, center of table)\n  ‚Ä¢ No scale bar or explicit measurement units are present.\n  ‚Ä¢ The robot arm is equipped with a gripper, poised to interact with the objects.\n\nSpatial Relationships :\n  ‚Ä¢ The hand and white mug are close together on the left side of the table.\n  ‚Ä¢ The block kettle is centrally located, with the robot arm approaching it.\n  ‚Ä¢ The robot arm is on the right, extending toward the objects, suggesting imminent grasping.\n  ‚Ä¢ The bounding boxes and labels are overlaid directly on the corresponding objects for clear identification.\n\nAnalysis :\n  ‚Ä¢ The vision detection module successfully identifies and localizes multiple objects of interest (hand, white mug, block kettle) with confidence scores above 0.5.\n  ‚Ä¢ The spatial arrangement suggests the robot is targeting the block kettle for grasping, as it is closest to the robot's gripper.\n  ‚Ä¢ The system demonstrates the ability to extract actionable information (object identity and pose) for robotic manipulation in a cluttered environment.",
      "grounding": [
        {
          "box": {
            "l": 0.05852164328098297,
            "t": 0.05885051190853119,
            "r": 0.49548614025115967,
            "b": 0.3241402506828308
          },
          "page": 3
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "8af71d35-7e80-42e7-a28c-43487aa5ba1a"
    },
    {
      "text": "the scene. Grounded-Segment-Anything<sup>58</sup> was successfully deployed for our language-to-vision module.",
      "grounding": [
        {
          "box": {
            "l": 0.06108735501766205,
            "t": 0.35783445835113525,
            "r": 0.49887198209762573,
            "b": 0.3850390315055847
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "09fa7230-b384-4778-8e2f-1f499fb8e693"
    },
    {
      "text": "The vision system generated a three-dimensional (3D) Voxel\nrepresentation that was effective at identifying object poses in our\nsetup (the used Grounding DINO detection module achieved an aver-\nage precision of 52.5 on the COCO zero-shot transfer benchmark). For\nexample, we found that the module was able to correctly identify the\nwhite cup we used 100% of the time under our experimental conditions.",
      "grounding": [
        {
          "box": {
            "l": 0.062125593423843384,
            "t": 0.3863914906978607,
            "r": 0.49807772040367126,
            "b": 0.466210275888443
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "dafe7e76-3eea-45ad-b344-8733fd7069d6"
    },
    {
      "text": "The 3D Voxel representation contained the meshes of various objects. From these meshes, target poses were extracted at a frequency of 1/3 Hz. In principle, the system should have been able to detect any object. In the pilot work, however, we established that the system would not always accurately identify the different objects associated with making hot beverages. This was often due to confusion between objects with similar shapes or objects absent from the training dataset. We also found that occlusion caused by the robot‚Äôs end-effector could sometimes result in inaccuracies in object detection and lead to errors when we used highly cluttered environments. For example, the mean successful identification rate for a white cup was ~90% at occlusion ratios between 20% and 30%, but decreased substantially at higher occlusion ratios (for example, to ~20% for occlusion ratios between 80% and 90%). We anticipate that improvements in computer vision will enhance the ability of robots to deal with even the most visually complex environments. However, the performance of the vision system was impressive, and we found that our system could cope well with relatively unconstrained environments if the identified issues (for example, using out-of-distribution objects) were avoided (Fig. 3).",
      "grounding": [
        {
          "box": {
            "l": 0.06254804134368896,
            "t": 0.46766197681427,
            "r": 0.49829262495040894,
            "b": 0.7273565530776978
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "edb80d8d-2da7-40e2-beab-90e3f36ec2c3"
    },
    {
      "text": "Force feedback  \nWe found that an ATI multiaxis force and torque sensor provided sufficient force feedback for skilful object interaction. The sensor provided six components of force and torque, and the forces exerted by the robot‚Äôs end-effector during task execution were successfully measured. We found that the sensor‚Äôs accuracy was within -2% of the full scale at a sampling rate of 100 Hz.",
      "grounding": [
        {
          "box": {
            "l": 0.06261338293552399,
            "t": 0.7388024926185608,
            "r": 0.4986001253128052,
            "b": 0.8333019614219666
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "a07925ac-f396-4354-a1f9-50ad77caac59"
    },
    {
      "text": "The robot was found to demonstrate a variety of motion dynamics\naccompanied by distinct types of force feedback during task execution.\nFigure 4 illustrates the forces experienced as the robot was preparing\ncoffee and handing over a pen. As shown in Fig. 4, a diverse spectrum\nof external forces was handled across various tasks. For example, when\nputting down a mug, the peak upward force was used as an indicator\nof successful placement. By contrast, during drawer manipulation, the\nforces and torques along the x and y axes were critical, highlighting",
      "grounding": [
        {
          "box": {
            "l": 0.06353738903999329,
            "t": 0.8343779444694519,
            "r": 0.49847283959388733,
            "b": 0.9437934756278992
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "3795a2a5-6152-4740-9c39-0450a1d3b6a4"
    },
    {
      "text": "their importance for successful task execution. The variability in force\nfeedback exemplifies the advantages of our scalable approach that\nadapts to the requirements of diverse motions.",
      "grounding": [
        {
          "box": {
            "l": 0.5126755833625793,
            "t": 0.05934976786375046,
            "r": 0.9463930726051331,
            "b": 0.10030809789896011
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "074dd0a5-6064-4839-83ac-803c141bb43d"
    },
    {
      "text": "The pouring accuracy achieved was -5.4 g per 100 g at a pitch\nvelocity of 4 m s‚Åª¬π. We assumed a quasi-static equilibrium to estimate\nthe volume of water poured at any given moment. However, as the pitch\nvelocity increased, the accuracy decreased, with errors approaching\n~20 g s‚Åª¬π at a pitch velocity of 30 m s‚Åª¬π. This decrease in accuracy can\nbe attributed to the breakdown of the quasi-static assumption and the\nimpact of the mass distribution of both pouring medium and container\non measurement accuracy.",
      "grounding": [
        {
          "box": {
            "l": 0.5114889144897461,
            "t": 0.10064749419689178,
            "r": 0.9456597566604614,
            "b": 0.20927570760250092
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "b635efbf-a57f-4d76-9927-9e6c75bd8ae3"
    },
    {
      "text": "Generating art  \nDALL-E<sup>59</sup> was found to successfully produce an image from which we could derive a drawing trajectory. It was found that this enabled the robot to draw any design specified by the user. We found that DALL-E was able to create silhouettes based on keywords extracted from the user, such as ‚Äòrandom bird‚Äô or ‚Äòrandom plant‚Äô. The silhouette‚Äôs outline was extracted and transformed to match the dimensions of the target surface. This allowed the robot to replicate the design on various physical objects (Fig. 5). We found that force feedback applied an even pen pressure when drawing, and this allowed control over the z component (Supplementary Section 2).",
      "grounding": [
        {
          "box": {
            "l": 0.5115026831626892,
            "t": 0.2228018045425415,
            "r": 0.9461794495582581,
            "b": 0.37289249897003174
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "db23817a-0619-43c5-9e68-bb04cfd1280f"
    },
    {
      "text": "Evaluation\nWe evaluated our method for generating robotic plans against Vox-Poser, which does not utilize RAG or force feedback. To compare the methods, we prompted an LLM to generate 80 human-like queries, reflecting the range of tasks specified in the knowledge base. These queries were then used to generate robot plans. We compared the performance outcomes from using RAG (our method)‚Äîin which the knowledge base is dynamically integrated into the LLM‚Äôs decision-making process‚Äîto a baseline (VoxPoser) in which the knowledge base was statically incorporated into the LLM‚Äôs context window. It is important to note that the second approach lacks scalability and becomes impractical as the knowledge base expands.",
      "grounding": [
        {
          "box": {
            "l": 0.5119324922561646,
            "t": 0.38675886392593384,
            "r": 0.9459419250488281,
            "b": 0.5474342703819275
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "717b59ec-5822-47ca-aeee-63a3bcfa3bf4"
    },
    {
      "text": "We evaluated the results based on answer faithfulness, which assesses an answer‚Äôs truthfulness and accuracy (ensuring factual representation without fabrication or ‚Äòhallucination‚Äô errors). In our findings, using RAG improved the faithfulness of responses. For GPT-4 (gpt-4-0613), the faithfulness score increased from 0.74 to 0.88 with RAG. Similarly, GPT-3.5-turbo (gpt-3.5-turbo-0125) achieved 0.86 with RAG compared with 0.78 without it, and Zephy-7B-beta saw an increase",
      "grounding": [
        {
          "box": {
            "l": 0.5127694010734558,
            "t": 0.5491029620170593,
            "r": 0.9450374245643616,
            "b": 0.644187867641449
          },
          "page": 3
        }
      ],
      "chunk_type": "text",
      "chunk_id": "2786a20c-fd64-42c2-a7c3-e45ec801c04c"
    },
    {
      "text": "Summary : This figure presents time-series plots of force (N), position (m), and velocity (m/s) feedback along the x, y, and z axes during a robot's coffee preparation task, segmented by different motion phases.\n\nline plot:\n# Title & Axes :\n  ‚Ä¢ Title: \"Force, velocity and position feedback. Force (N), velocity (m s‚Åª¬π) and position (m) plots during the robot‚Äôs coffee preparation, illustrating the diverse force feedback across the different motions.\"\n  ‚Ä¢ X-axis: Time (no explicit units or tick labels shown; time increases left to right).\n  ‚Ä¢ Y-axes: \n    ‚Äì Top panel: Force (N), range approximately -30 to +30.\n    ‚Äì Middle panel: Position (m), range approximately -0.8 to +0.8.\n    ‚Äì Bottom panel: Velocity (m/s), range approximately -0.75 to +0.75.\n  ‚Ä¢ No explicit tick labels shown for time; y-axis tick marks are visible but not numerically labeled.\n\n# Data Series :\n  ‚Ä¢ Three variables plotted in each panel, corresponding to spatial axes:\n    ‚Äì x axis: black line\n    ‚Äì y axis: blue line\n    ‚Äì z axis: red line\n\n# Design Encodings :\n  ‚Ä¢ Colour coding: black (x), blue (y), red (z).\n  ‚Ä¢ Three vertically stacked panels: force (top), position (middle), velocity (bottom).\n  ‚Ä¢ Background shading segments the timeline into seven labelled phases:\n    ‚Äì Open door 1 (white)\n    ‚Äì Open door 2 (light blue)\n    ‚Äì Put mug down (light green)\n    ‚Äì Scoop coffee (light pink)\n    ‚Äì Empty in cup (light yellow)\n    ‚Äì Pour water (light orange)\n    ‚Äì Pen handover (light purple)\n  ‚Ä¢ Legend at top right for axis colours.\n  ‚Ä¢ No markers; all lines are solid.\n\n# Motion Phases (Background Segments) :\n  ‚Ä¢ Open door 1\n  ‚Ä¢ Open door 2\n  ‚Ä¢ Put mug down\n  ‚Ä¢ Scoop coffee\n  ‚Ä¢ Empty in cup\n  ‚Ä¢ Pour water\n  ‚Ä¢ Pen handover\n\n# Distribution & Trends :\n  ‚Ä¢ Force: Large oscillations in x and y axes during \"Open door 2\"; force stabilises in later phases.\n  ‚Ä¢ Position: Gradual changes, with more pronounced shifts during \"Put mug down\" and \"Empty in cup\".\n  ‚Ä¢ Velocity: Spikes and oscillations align with transitions between motion phases, especially during \"Open door 2\" and \"Put mug down\".\n\n# Analysis :\n  ‚Ä¢ The robot experiences the most dynamic force and velocity feedback during \"Open door 2\", as shown by high-frequency oscillations in the x and y axes.\n  ‚Ä¢ Position changes are more gradual, with step-like transitions at phase boundaries.\n  ‚Ä¢ The z axis (red) shows distinct force and position changes during \"Scoop coffee\" and \"Empty in cup\", indicating vertical movements.\n  ‚Ä¢ Each motion phase is associated with a unique pattern of force, position, and velocity feedback, reflecting the physical demands of each task segment.\n  ‚Ä¢ The drawing component is omitted for clarity, as noted in the caption.",
      "grounding": [
        {
          "box": {
            "l": 0.5094380378723145,
            "t": 0.6708055138587952,
            "r": 0.9407781362533569,
            "b": 0.9421586394309998
          },
          "page": 3
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "0e32d04f-0157-43e4-beb3-4761f3c6df67"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.06437326967716217,
            "t": 0.9603433012962341,
            "r": 0.4487060308456421,
            "b": 0.9759928584098816
          },
          "page": 3
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "1e7f8a3f-659e-4151-8d58-34d0625730ac"
    },
    {
      "text": "595",
      "grounding": [
        {
          "box": {
            "l": 0.9171670079231262,
            "t": 0.9634062051773071,
            "r": 0.9477192759513855,
            "b": 0.9762502908706665
          },
          "page": 3
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "e2678a97-5582-485b-b36e-80afc645503d"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.06127525120973587,
            "t": 0.02568972110748291,
            "r": 0.12722685933113098,
            "b": 0.045186661183834076
          },
          "page": 4
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "64b23470-8bba-4e08-b069-83b390627b3d"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6496649980545044,
            "t": 0.025903822854161263,
            "r": 0.9486836194992065,
            "b": 0.04568473994731903
          },
          "page": 4
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "d212b5cb-9caa-4371-8085-4d3b28e720c0"
    },
    {
      "text": "Summary : This figure illustrates a three-step drawing process for generating and physically rendering images of a random animal, food, and plant. Each row shows (1) the generated image, (2) a contour plot with drawing path, and (3) the final drawing on a physical plate.\n\nphoto and process diagram:\n# Overview :\n  ‚Ä¢ Three rows, each corresponding to a different category: random animal (bird), random food (apple), and random plant (clover).\n  ‚Ä¢ Each row contains three columns: generated image, contour plot of drawing path, and photo of the drawing rendered on a plate.\n\n# Row 1: Random Animal (Bird) :\n  ‚Ä¢ Left: Black silhouette of a bird in flight, facing right.\n  ‚Ä¢ Middle: Contour plot with grid and numbered points tracing the outline of the bird, showing the drawing sequence.\n  ‚Ä¢ Right: Photo of a plate with a green line drawing of the bird, matching the generated image's outline.\n\n# Row 2: Random Food (Apple) :\n  ‚Ä¢ Left: Black silhouette of an apple with a leaf.\n  ‚Ä¢ Middle: Contour plot with grid and numbered points tracing the apple's outline, including the leaf and stem.\n  ‚Ä¢ Right: Photo of a plate with a red line drawing of the apple, closely following the generated image.\n\n# Row 3: Random Plant (Clover) :\n  ‚Ä¢ Left: Black silhouette of a three-leaf clover.\n  ‚Ä¢ Middle: Contour plot with grid and numbered points tracing the clover's outline, including the stem and three leaves.\n  ‚Ä¢ Right: Photo of a plate with a blue line drawing of the clover, matching the generated image.\n\n# Technical Details :\n  ‚Ä¢ All generated images are black silhouettes on a white background.\n  ‚Ä¢ Contour plots use purple lines and numbered points to indicate drawing order and path.\n  ‚Ä¢ Plate drawings use single-color lines (green, red, blue) corresponding to each category.\n  ‚Ä¢ Plates are photographed from above, with the drawing centered.\n\n# Spatial Relationships :\n  ‚Ä¢ Each row presents a left-to-right sequence: digital image ‚Üí drawing path ‚Üí physical rendering.\n  ‚Ä¢ The physical drawings on plates closely match the contours of the generated images.\n\n# Analysis :\n  ‚Ä¢ The process demonstrates high fidelity between the generated image, the planned drawing path, and the final physical rendering.\n  ‚Ä¢ The contour plots reveal the sequential logic used to trace each shape.\n  ‚Ä¢ The approach is consistent across different object categories (animal, food, plant), indicating a generalizable drawing method.\n  ‚Ä¢ The physical renderings on plates are clear and accurately follow the digital outlines, validating the effectiveness of the drawing process.",
      "grounding": [
        {
          "box": {
            "l": 0.06497973203659058,
            "t": 0.06296536326408386,
            "r": 0.9460510611534119,
            "b": 0.5875606536865234
          },
          "page": 4
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "8848d972-5082-4539-ad73-75c0ba929387"
    },
    {
      "text": "from 0.37 to 0.44. The improvement in faithfulness is particularly key\nfor robotic applications, where accurate execution during physical\ninteractions is essential.",
      "grounding": [
        {
          "box": {
            "l": 0.06197662651538849,
            "t": 0.614909827709198,
            "r": 0.4992526173591614,
            "b": 0.6590630412101746
          },
          "page": 4
        }
      ],
      "chunk_type": "text",
      "chunk_id": "020a0d3c-bbdf-4e40-a70d-37c9fb631251"
    },
    {
      "text": "## Discussion\nWe tested our methodology‚Äîthe ELLMER framework‚Äîthat combines techniques from artificial intelligence and robot manipulation to create an intelligent robot. Our approach successfully combined the cognitive abilities of LLMs with the sensorimotor skills of robots, enabling our robot to interpret a high-order verbal command and execute a complex long-horizon task while adeptly managing uncertainties. We used the LLM, augmented with feedback loops and RAG, to write expressive code and facilitate the manipulation sub-tasks required by the robot to achieve the high-level goal (making a hot beverage). ELLMER allowed real-time adaptation to environmental changes and leveraged a repository of precise solutions via RAG. This ensured accurate task execution and broad adaptability32.",
      "grounding": [
        {
          "box": {
            "l": 0.06255047023296356,
            "t": 0.6680896878242493,
            "r": 0.4988522529602051,
            "b": 0.8473449349403381
          },
          "page": 4
        }
      ],
      "chunk_type": "text",
      "chunk_id": "a783fd90-c2de-457f-915f-89ca154b23ff"
    },
    {
      "text": "ELLMER encoded known constraints into the code examples (‚Äòmotion functions‚Äô) and enabled rapid accommodation to numerous uncertainties, such as fluctuating ingredient quantities or opening unknown drawers‚Äîcapabilities that other methods lack without extensive additional training29,33,60,61. The integration of vision, force and language modalities enhanced the manipulation performance. The force sensors improved task precision (for example, pouring a precise",
      "grounding": [
        {
          "box": {
            "l": 0.06376951932907104,
            "t": 0.8482388854026794,
            "r": 0.4984452724456787,
            "b": 0.9432291388511658
          },
          "page": 4
        }
      ],
      "chunk_type": "text",
      "chunk_id": "689dbe52-13e8-4435-a645-6c64a43b8b66"
    },
    {
      "text": "and accurate amount of liquid when vision was occluded), whereas the\nvision system identified object positions and movements. The language\ncapabilities enabled the system to produce feedback within the code,\nwhich is critical for adjusting to new tasks. The curated knowledge base\nimproved the LLM‚Äôs performance by tailoring information retrieval to\nthe specific task specifications, and this ensured high-quality contextu-\nally relevant outputs. A curated knowledge base is a pragmatic element\nthat enhances controllability, accuracy and scalability. In this context,\nRAG can be seen as providing a cultural milieu of knowledge from which\na robot can draw. In particular, this mirrors the ‚Äòintelligence‚Äô afforded\nto humans through the cultural transmission of knowledge. Thus, our\nwork shows that integrating advanced language models and sensori-\nmotor control strategies allows robots to leverage the exponential\nadvancements in LLMs, enabling more sophisticated interactions.\nThis will usher in the next age of automation with unprecedented levels\nof autonomy and precision, accentuating the need to manage these\nadvancements safely62.",
      "grounding": [
        {
          "box": {
            "l": 0.5109907388687134,
            "t": 0.6180933713912964,
            "r": 0.9467223882675171,
            "b": 0.8471311330795288
          },
          "page": 4
        }
      ],
      "chunk_type": "text",
      "chunk_id": "31b0e1b3-7317-4355-b5d0-706dcab26d22"
    },
    {
      "text": "ELLMER‚Äôs potential extends to creating intricate and artistic movements. For instance, a model like DALL¬∑E allows trajectories to be derived from visual inputs and opens new avenues for robotic trajectory generation. This method can be widely applied in tasks such as cake decoration or latte art. In future work, incorporating queries and images will enable novel trajectory generation, allowing for increased versatility. Moreover, recent LLM enhancements are",
      "grounding": [
        {
          "box": {
            "l": 0.5121976733207703,
            "t": 0.8482103943824768,
            "r": 0.946309506893158,
            "b": 0.9435011744499207
          },
          "page": 4
        }
      ],
      "chunk_type": "text",
      "chunk_id": "9c88aa9c-83d9-4920-9be6-2503a2304b90"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.06384608149528503,
            "t": 0.9603859186172485,
            "r": 0.45005545020103455,
            "b": 0.975853681564331
          },
          "page": 4
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "b19cd3b2-32a8-400a-8ae2-adf32528b295"
    },
    {
      "text": "596",
      "grounding": [
        {
          "box": {
            "l": 0.9147399663925171,
            "t": 0.9624037742614746,
            "r": 0.9475936889648438,
            "b": 0.9763563871383667
          },
          "page": 4
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "86537c44-94d0-4ad7-ba3a-f539ec5114f9"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.06094241887331009,
            "t": 0.025772906839847565,
            "r": 0.12689396739006042,
            "b": 0.045116834342479706
          },
          "page": 5
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "69d7ac0d-b604-46e3-b2c7-44914609519a"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.648370623588562,
            "t": 0.025254590436816216,
            "r": 0.9483520984649658,
            "b": 0.045624807476997375
          },
          "page": 5
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "9e7245df-4468-4698-bfc3-b1c355f78906"
    },
    {
      "text": "Summary : This photograph shows a Kinova Gen3 robotic arm in the process of preparing coffee and decorating a plate, as part of a demonstration of robotic manipulation in a kitchen-like environment.\n\nphoto:\nScene Overview :\n  ‚Ä¢ Main subject is a Kinova Gen3 robotic arm positioned on the right side of a white table.\n  ‚Ä¢ The robot is holding a black-handled tool or utensil, interacting with a plate that has a pastry and a cup of coffee.\n  ‚Ä¢ The setting resembles a kitchen or laboratory workspace, with a coffee maker, a set of drawers, and various containers (including a jar of instant coffee, a sugar jar, and a cup with stirring sticks).\n  ‚Ä¢ The lighting is bright and even, with a neutral colour palette dominated by white, black, and metallic tones.\n\nTechnical Details :\n  ‚Ä¢ No scale bar or magnification is present.\n  ‚Ä¢ The plate on the table has a blue line drawing of a bird as decoration.\n  ‚Ä¢ The coffee cup is white with a blue rim, and the pastry is placed on a purple plate with a white pattern.\n  ‚Ä¢ The robot is a Kinova Gen3, identifiable by its white casing and blue logo.\n\nSpatial Relationships :\n  ‚Ä¢ The robot is on the right, reaching toward the center of the table.\n  ‚Ä¢ The coffee maker and drawers are on the left side of the image.\n  ‚Ä¢ The decorated plate and coffee cup are centrally placed, with other items (jars, utensils) arranged nearby.\n\nAnalysis :\n  ‚Ä¢ The image visually demonstrates the robot‚Äôs ability to perform multi-step tasks such as preparing coffee and decorating a plate, highlighting dexterity and workspace organization. The scene is staged to showcase both the robot and the completed food items, emphasizing the integration of robotics into everyday kitchen activities.",
      "grounding": [
        {
          "box": {
            "l": 0.06009262800216675,
            "t": 0.057796038687229156,
            "r": 0.4987267851829529,
            "b": 0.2754606604576111
          },
          "page": 5
        }
      ],
      "chunk_type": "figure",
      "chunk_id": "31b848d5-0a8e-4612-8d7f-b387660b1e61"
    },
    {
      "text": "set to notably improve the fluidity and effectiveness of human‚Äìrobot\ninteractions. Our examples of coffee making and plate decoration\nrepresent only a subset of the complex task types that a sophisticated\nrobot might be required to undertake. ELLMER is conducive to being\nscaled up, so it includes a wide range of possible long-horizon tasks.\nThus, ELLMER could incorporate a database of feedback loops or\n‚Äòlearning-from-demonstration‚Äô examples to facilitate a wide variety\nof complex robotic manipulations.",
      "grounding": [
        {
          "box": {
            "l": 0.05998200178146362,
            "t": 0.3031555414199829,
            "r": 0.5001221895217896,
            "b": 0.41250425577163696
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "293ffbb0-cdb3-4a66-a1fc-975294ccccea"
    },
    {
      "text": "ELLmER is based on two assumptions concerning computer vision: (1) the vision module accurately identifies and classifies objects within the scene and (2) a comprehensive affordance map of the utensils is available. We endowed our model with prior knowledge of the kettle, spoon and door handle affordances, but recent work suggests that affordances can be learned with minimal data63,64. Our focus was not on object detection, but we noted that detection response times hindered optimal performance. In addition, ELLMER could adjust to real-time changes but struggled with proactive adaptations (for example, task switching midway without prior programming). In future iterations, more frequent querying of the language model would allow the reassessment and modification of overall plans based on new inputs. We also note that there are still challenges that need to be addressed, such as the sophisticated modelling of complex force dynamics (for example, the forces on the end-effector as a function of the flow rate, container size and liquid viscosity) and the integration of spatial awareness tools (such as OctoMaps, a robotic library for a 3D occupancy map). Incorporating tactile sensors and using soft robotic techniques would improve the robot‚Äôs ability to apply appropriate forces without causing damage. ELLMER provides a flexible platform for incorporating these research developments, enabling robots to use ‚Äòsensory‚Äô feedback to interpret material properties and precisely tailor the forces they apply.",
      "grounding": [
        {
          "box": {
            "l": 0.061365410685539246,
            "t": 0.4135684669017792,
            "r": 0.49850666522979736,
            "b": 0.7113029956817627
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "4a876d32-6323-423f-8e1e-c29dc47e0e88"
    },
    {
      "text": "The current iteration of ELLMER allowed the robot to successfully\ncomplete a complex task in ‚Äòone shot‚Äô. This provides a compelling\npicture of the capabilities of intelligent machines that combine sen-\nsorimotor capabilities with the abstract reasoning provided by LLMs.\nNevertheless, we anticipate that the robot capacity will increase expo-\nnentially as the components combined within ELLMER become ever\nmore refined. Our framework is hardware agnostic and can be easily\ncustomized with open-source RAG solutions like Haystack, supporting\nquick adjustments to embedders, retrievers, chunking techniques and\nLLMs. ELLMER offers a flexible framework for researchers to collabora-\ntively develop intelligent machines. Supplementary Section 3 provides\nmore information on ELLMER and future research.",
      "grounding": [
        {
          "box": {
            "l": 0.06287343800067902,
            "t": 0.7120341062545776,
            "r": 0.49778664112091064,
            "b": 0.8743234872817993
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "37a45eaa-92b1-4b84-ab0e-39f632ee8b31"
    },
    {
      "text": "The power of our approach lies in the embodiment of cognition through a framework that combines enhanced sensorimotor abilities with the cognitive reasoning capabilities of LLMs. Through this combination, ELLMER enables robots to explore and interact with their environment more effectively, emulating aspects of the connection between",
      "grounding": [
        {
          "box": {
            "l": 0.06407177448272705,
            "t": 0.8753491044044495,
            "r": 0.49802422523498535,
            "b": 0.9433837532997131
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "ab51da6f-0c94-459c-8932-619c12cb3e0d"
    },
    {
      "text": "experience and action observed in human intelligence. This opens up\nopportunities for robots to gain a form of ‚Äòphysical intelligence‚Äô, where\ntheir exploration of the environment drives the sensorimotor learn-\ning process. In conclusion, ELLMER integrates language processing,\nRAG, force and vision to enable robots to adapt to complex tasks. It\ncombines the following features: (1) interpreting high-level human com-\nmands, (2) completing long-horizon tasks and (3) utilizing integrated\nforce and vision signals to manage noise and disturbances in changing\nenvironments. ELLMER allows methods such as reinforcement learn-\ning, imitation learning and flexible motion primitives to be combined\nholistically for enhanced adaptability and ‚Äòrobot intelligence‚Äô in diverse\nand dynamic scenarios. It demonstrates that integrating the cognitive\nreasoning capabilities of LLMs with robots‚Äô sensorimotor skills allows\nthem to interpret and manipulate their environment and complete\ncomplex tasks through embodied machine intelligence.",
      "grounding": [
        {
          "box": {
            "l": 0.5102868676185608,
            "t": 0.05893237143754959,
            "r": 0.9468488097190857,
            "b": 0.26465320587158203
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "b08855b5-6d96-47ac-96d8-8aaccc0d7a6e"
    },
    {
      "text": "## Methods\n### Overview\nThe goal of the robot was to respond to high-level human commands in a dynamic environment, such as a home kitchen. We designed a realistic setting featuring items including a kettle, white mug, drawers, kitchen paraphernalia and a coffee pot. The scenario was designed to test the robot‚Äôs ability to perform diverse tasks in a realistic, although reasonably constrained, environment as it interacts with a human present. We assumed that robotic low-level control mechanisms managed obstacle avoidance. The pipeline consisted of a language-processing component for task execution, a vision system for pose detection and a force module for object manipulation. All of this was integrated within a robotic operating system (ROS) process.",
      "grounding": [
        {
          "box": {
            "l": 0.5105847120285034,
            "t": 0.27559971809387207,
            "r": 0.9463620185852051,
            "b": 0.4526856541633606
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "dc2eadf8-7410-47c4-8dce-0918907d0f29"
    },
    {
      "text": "Specifically, our approach built on the ‚Äòcode for dynamic policies‚Äô approach‚Å∂‚Åµ that can facilitate adaptable robotic actions. In our implementation, we utilized GPT-4 and OpenAI‚Äôs RAG infrastructure. We leveraged LLMs‚Äô capabilities using RAG¬≥¬≤ to dynamically select and adapt the most suitable policy from a database or generate its own code based on relevant examples. In contrast to existing pure LLM-driven methods¬≤‚ÅµÀí¬≤‚Å∑Àí¬≤‚Åπ, we integrated force and vision into the framework, allowing the system to adapt to a variety of complex tasks in dynamic settings. This approach equips the robotic system with the capacity for high-level contextual understanding¬≤‚Åµ and the proficiency to execute complex tasks with real-time feedback, ensuring accuracy and precision. The approach ensures that each action is aligned with the specific demands of the task and the environmental conditions (Fig. 6).",
      "grounding": [
        {
          "box": {
            "l": 0.5113515853881836,
            "t": 0.4537847340106964,
            "r": 0.9455054998397827,
            "b": 0.6312656402587891
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "e6826367-2276-491a-a4f5-254487726f58"
    },
    {
      "text": "Hardware and software\nA Kinova seven-degrees-of-freedom robot was used. An Azure Kinect Sensor was used at a resolution of 640 √ó 576 px¬≤ and 30 fps, along with an ATI multiaxis force sensor. A140-mm Robotiq gripper was attached to the end of the robot. The force sensor was attached to the Robotiq gripper and Kinova arm using a 3D printed flange. A small cylinder was placed on the force sensor on the side closest to the gripper so that the movements of the gripper would not touch the force sensor, leading to readings being inaccurate. A Dell desktop computer with an Intel Core i9 processor with an NVIDIA RTX 2080 graphics-processing unit was used and connected to the robot with an Ethernet cable. Similarly, both Azure cameras were attached to the desktop. Ubuntu 20.04 and the ROS were used. Our code relied on the Kinova ROS Kortex library. The NVIDIA RTX 2080 utilizes -225 W under typical load conditions‚Å∂‚Å∂, whereas the Kinova robotic arm consumes -36 W (ref. 67). In our scenarios, each task runs for up to 4 min. Utilizing the EPA‚Äôs average conversion factor of -0.4 kg of CO‚ÇÇ per kWh for mixed energy sources‚Å∂‚Å∏, the carbon emission for each task comes to -0.007 kg (7 g) of CO‚ÇÇ.",
      "grounding": [
        {
          "box": {
            "l": 0.5114344358444214,
            "t": 0.6437723636627197,
            "r": 0.9460834264755249,
            "b": 0.8881607055664062
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "4aea81a5-e567-4d72-8dec-63a1ef9cff5c"
    },
    {
      "text": "**Language processing**\nThe LLM processes an image and the user‚Äôs query, systematically breaking down the complex task $L_T$ into a sequence of steps $\\{L_1, L_2, ..., L_n\\}$,",
      "grounding": [
        {
          "box": {
            "l": 0.5120793581008911,
            "t": 0.9017558097839355,
            "r": 0.9448184967041016,
            "b": 0.9455137252807617
          },
          "page": 5
        }
      ],
      "chunk_type": "text",
      "chunk_id": "b72e392a-2d29-466d-8880-e9b461881559"
    },
    {
      "text": "[**Nature Machine Intelligence**](https://www.nature.com/natmachintell) | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.06368790566921234,
            "t": 0.9599875807762146,
            "r": 0.4498385787010193,
            "b": 0.9767743945121765
          },
          "page": 5
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "04bdb4fe-5734-44d8-a01f-293c7b93d2f9"
    },
    {
      "text": "597",
      "grounding": [
        {
          "box": {
            "l": 0.9179518818855286,
            "t": 0.9625405669212341,
            "r": 0.9490613341331482,
            "b": 0.9768827557563782
          },
          "page": 5
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "e634a184-f336-4e69-9b5c-2bcc29a3b7eb"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.06181186065077782,
            "t": 0.026199571788311005,
            "r": 0.12624728679656982,
            "b": 0.04433716833591461
          },
          "page": 6
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "1fe33de3-9658-49f9-9f73-fc67584b00e0"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6490153074264526,
            "t": 0.026032935827970505,
            "r": 0.9478400945663452,
            "b": 0.04501200094819069
          },
          "page": 6
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "d22b9f21-232d-4d03-b1bd-ffc63bddb98a"
    },
    {
      "text": "where each step $L_i$ may depend on the completion of the preceding\nsteps. The sequence of steps is critical, and dependencies exist between\nsteps; for example, if an object (for example, a mug) is required but not\nfound, then potentially a cupboard should be opened.",
      "grounding": [
        {
          "box": {
            "l": 0.06269913911819458,
            "t": 0.058279965072870255,
            "r": 0.4990444779396057,
            "b": 0.11376000940799713
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "07984a8d-c977-4340-92a4-dbcdd3234ab2"
    },
    {
      "text": "The environmental data gathered from the initial image input are key in decomposing the abstract task. For instance, when asked to make a beverage, the ingredients present in the environment are critical in deciding which drink to make, and the visual information can help identify possible locations. The interface was facilitated by GPT-4, which ran under the instruction to write and dispatch code to a robot via the server platform. The process was assisted by a knowledge base containing code examples and allowed continuous communication with the robot. The curated knowledge base contained validated examples of low- and high-order actions that incorporate known uncertainties. Including these motion examples is key to enabling the robot to handle numerous scenarios and complete long-horizon tasks. High-level motion primitives or policies can compress multiple known uncertainties into a single function, reducing the need for extensive code writing. RAG allowed the knowledge base to be comprehensive without sacrificing performance. The system interacted with the ROS and communicated via a low-latency connection provided by the EC2 server through JSON action queries and responses.",
      "grounding": [
        {
          "box": {
            "l": 0.06271786987781525,
            "t": 0.11496998369693756,
            "r": 0.49851006269454956,
            "b": 0.3579062223434448
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "e6aaf132-6202-4b42-bd80-88ddf61da2b7"
    },
    {
      "text": "The dependency among tasks is expressed through conditional probabilities such as $P(L_{2A}, L_{2B}|L_1)$, which specifies the likelihood of progressing to tasks $L_{2A}$ or $L_{2B}$ following the successful execution of task $L_1$. This helps in planning the sequence of steps, ensuring the robot can adapt its actions based on real-time feedback. The LLM generates executable code that is sent to the server, based on the instructions (prompt) and a knowledge base containing examples. The code is run on the ROS in a secure environment that only has access to predefined functions, thereby ensuring safety in task execution.",
      "grounding": [
        {
          "box": {
            "l": 0.06332200765609741,
            "t": 0.35869669914245605,
            "r": 0.4979580044746399,
            "b": 0.48162609338760376
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "21a150d0-1954-40cb-afc8-8d2863c587ac"
    },
    {
      "text": "**RAG**\nA key feature of our system is the deployment of RAG. RAG integrates user queries with information from a continually updated, curated knowledge base, optimizing the output of the LLM. This approach allows the model to follow code examples provided in the database, ensuring accuracy, reliability and scalability as the knowledge base evolves.",
      "grounding": [
        {
          "box": {
            "l": 0.0630912035703659,
            "t": 0.49384960532188416,
            "r": 0.4975113868713379,
            "b": 0.5757296681404114
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "01bc99fe-7ffe-41d3-8589-4cf3a60ad9b5"
    },
    {
      "text": "We used vector RAG, which involves using an encoder to embed\nthe query ($q$) and segments of the knowledge base ({$s_1, s_2, ..., s_m$}), known\nas chunks, into vector representations. Chunks were then compared\nwith the query based on cosine similarity, and the top $k$ chunks were\nselected as contextually relevant information for generating responses.\nAlternative retrieval techniques that can be used within our framework\ninclude traditional RAG (keyword-/rule-based RAG) or hybrid retrieval\nmethods.",
      "grounding": [
        {
          "box": {
            "l": 0.06382131576538086,
            "t": 0.5760967135429382,
            "r": 0.4971051812171936,
            "b": 0.6840526461601257
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "8eb33839-3d90-4062-9994-b2905455ec7f"
    },
    {
      "text": "The RAG pipeline can be customized by selecting different document stores (the medium in which the knowledge base is stored and organized). In our experimental test, we used the inbuilt OpenAI RAG process and organized our curated knowledge base in a markdown file as the document store. However, a range of other RAG approaches can be used in our framework, utilizing tools like Haystack^56 and Vebra^57. These tools allow users to select a range of document stores‚Äîfrom ‚Äòmarkdown files‚Äô for simple text-based knowledge to ‚ÄòElasticsearch‚Äô for complex, indexed data‚Äîalong with specific embedders, retrievers and chunking techniques, as well as the LLM itself.",
      "grounding": [
        {
          "box": {
            "l": 0.06349669396877289,
            "t": 0.6853432655334473,
            "r": 0.49713534116744995,
            "b": 0.8207515478134155
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "147e6e78-dcce-47bd-8ac0-b05760426c96"
    },
    {
      "text": "Vision system  \nGrounded-Segment-Anything was used as the language-to-vision model to create a 3D voxel that highlighted the positions of all objects and allowed their poses to be extracted for robotic grasping$^{58,69}$. This enabled (1) the generation of object-specific bounding boxes, (2) the manufacture of segmented masks via MobileSAM and (3) the creation of voxels that encapsulate the detected objects. The voxels allowed target object poses to be extracted.",
      "grounding": [
        {
          "box": {
            "l": 0.06393595039844513,
            "t": 0.8337182998657227,
            "r": 0.4977155327796936,
            "b": 0.9432966709136963
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "fabb5d25-5a65-4afa-bab7-57c48fd5570a"
    },
    {
      "text": "Force module\nTo ensure accurate measurements in force-rich applications, we calibrated the ATI force sensor to compensate for gravitational forces, ensuring it registered zero in the absence of external forces. This calibration is key for accurately predicting the external forces applied to the end-effector. The process involved sequentially zeroing the force sensor on one axis, rotating the sensor and then zeroing on the next axis. The local forces were transformed into the global plane to estimate the upward force at different rotations $F_{global} = T_{end\\ effector\\ to\\ robot\\ base} \\times$",
      "grounding": [
        {
          "box": {
            "l": 0.5125311017036438,
            "t": 0.05958540737628937,
            "r": 0.9447665810585022,
            "b": 0.18099185824394226
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "7f514b1c-a80c-4b33-a920-1178ccd67a76"
    },
    {
      "text": "$_{global}^{T}end\\_effector\\_to\\_robot\\_base F_{local}$, where $F_{global}$ is the force vector in the global (robot base) coordinate frame, $T_{end\\_effector\\_to\\_robot\\_base}$ is the transformation matrix from the end-effector‚Äôs frame to the robot‚Äôs base frame and $F_{local}$ is the force vector in the local coordinate frame of the end-effector. We explored various methods, such as moving the sensor‚Äôs position and orientation and using polynomial functions for calibration. However, the simpler calibration method was found to be the most effective.",
      "grounding": [
        {
          "box": {
            "l": 0.5126316547393799,
            "t": 0.17759889364242554,
            "r": 0.9440120458602905,
            "b": 0.2765456438064575
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "98c216cd-a12b-4c53-87ad-8594c5f20bc2"
    },
    {
      "text": "To estimate the flow rates, we assumed a condition of static equilibrium and maintain slow operational speeds during pouring. Mathematically, this is represented as $F_{up} \\approx mg$ and $\\Delta F_{up} \\approx \\Delta mg$. In situations involving variable acceleration, the relationship between forces and flow rates becomes more complex. It necessitates a dynamic model that accounts for varying inputs, such as flow rates, container‚Äôs centre of mass and inertia of the end-effector, to map dynamic force inputs to the pouring flow rates.",
      "grounding": [
        {
          "box": {
            "l": 0.5126854777336121,
            "t": 0.27811503410339355,
            "r": 0.9439137578010559,
            "b": 0.3855510354042053
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "a9f568c1-045f-461c-9045-040623ab2123"
    },
    {
      "text": "The system continuously manages force vectors along three axes,\nadjusting the applied force based on the criteria within its knowledge\nbase. The LLM dynamically selects the necessary force magnitudes and\ndirections tailored to meet specific downstream task requirements. For\nexample, the knowledge base may specify varying force magnitudes to\nbe applied depending on the object characteristics or task demands.\nThis approach enables the system to adjust its actions autonomously\nto align with a broad range of operational criteria.",
      "grounding": [
        {
          "box": {
            "l": 0.5124517679214478,
            "t": 0.38691794872283936,
            "r": 0.9444435834884644,
            "b": 0.4946115016937256
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "b986bb2b-abcc-4b12-9a94-6c62feb71349"
    },
    {
      "text": "ROS operation  \nIn this work, we initiated the robotic processes by launching a Kinova ROS Kortex driver. This established a node that enables communication within the ROS network and the Kinova Gen3 robot. The node publishes several topics that subscribers can access, and it provides services that can be called to modify the robot‚Äôs configuration. The base joints are updated at a frequency of 40 Hz. Concurrently, the Robotiq 2F-140 mm gripper node is activated at 50 Hz. The node sets up a communication link with the gripper via a USB connection, and it initiates an action server that enables precise control of the gripper and facilitates the exchange of operating data.",
      "grounding": [
        {
          "box": {
            "l": 0.5124484896659851,
            "t": 0.5088728070259094,
            "r": 0.9442302584648132,
            "b": 0.6562328934669495
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "d80ea394-7e94-46af-822a-3d477f742d08"
    },
    {
      "text": "A vital element of our robotic system is the vision module node.\nA ‚Äòclasses‚Äô variable is used to identify the target pose of selected objects\nwithin the environment. This variable can be dynamically updated,\nthereby allowing the system to adapt to changes in the scene. The pose\ncoordinates of the objects, as established by the ‚Äòclasses‚Äô variable, are\npublished approximately at every ~ $\\frac{1}{3}$ Hz. This is largely due to the\nprocessing time of Grounding DINO in detecting objects and establish-\ning the bounding boxes. Moreover, we used an AprilTag to determine\nthe position of the camera relative to the robot‚Äôs base. This is repre-\nsented as $PR = T_{AR} \\times (T_{CA} \\times P^C)$, where $P^C$ is the point in the camera frame,\n$T_{CA}$ is the transformation matrix from the camera frame to the AprilTag,\n$T_{AR}$ is the transformation matrix from the AprilTag to the robot‚Äôs\nbase and PR is the point in the robot‚Äôs base frame.",
      "grounding": [
        {
          "box": {
            "l": 0.5122247338294983,
            "t": 0.6582398414611816,
            "r": 0.9440091252326965,
            "b": 0.8325775861740112
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "1f4da0b9-56e8-4bec-a992-8211dc401749"
    },
    {
      "text": "In parallel, a force node is launched at a frequency of 100 Hz and provides multiaxis force and torque readings, localized to the ATI force transducer. The readings are transformed using a quaternion-based 3 √ó 3 rotation matrix to align with the global base frame of the robot, providing raw and averaged values over the last five time steps across fixed degrees of freedom. It calculates forces in the global frame of the robot base using the rotational matrix, calculated from the kinematic data.",
      "grounding": [
        {
          "box": {
            "l": 0.5130150318145752,
            "t": 0.8347891569137573,
            "r": 0.9443690776824951,
            "b": 0.9421491622924805
          },
          "page": 6
        }
      ],
      "chunk_type": "text",
      "chunk_id": "11133c44-208a-4f6a-85f5-f55b7d3197b4"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.06459489464759827,
            "t": 0.9603434205055237,
            "r": 0.44868943095207214,
            "b": 0.976327121257782
          },
          "page": 6
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "21694e65-ea03-4a48-8915-c8b26098170c"
    },
    {
      "text": "598",
      "grounding": [
        {
          "box": {
            "l": 0.91544508934021,
            "t": 0.9626778364181519,
            "r": 0.947076678276062,
            "b": 0.9761605262756348
          },
          "page": 6
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "d1c6e9b7-8e3e-4b73-8f05-303e63ecc135"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.060966674238443375,
            "t": 0.025477685034275055,
            "r": 0.1272054761648178,
            "b": 0.04530245065689087
          },
          "page": 7
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "4dd34b43-9373-4299-a51f-69744f6ec8c5"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6477341651916504,
            "t": 0.02538842335343361,
            "r": 0.9486006498336792,
            "b": 0.04549675062298775
          },
          "page": 7
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "887be186-cec8-4c33-a33f-92e14807aeea"
    },
    {
      "text": "ROS facilitates the continuous processing of multimodal feedback\ndata from the language processing, vision systems, force metrics and\njoint end-effector positions. The motions operate on a foundational\nsix-degrees-of-freedom twist command, which controls velocity and\nthe variable speed and force gripper procedures for opening and clos-\ning. This enables the integration of hard-coded safety constraints, such\nas maximum velocity and force limits, as well as workspace boundaries.\n\nThe linear velocities were clamped within ¬±0.05 m s‚Åª¬π and the\nangular velocities were clamped within ¬±60¬∞ s‚Åª¬π. End-effector forces\nwere also limited to 20 N. This is coded into the fundamental motion\nprimitives; therefore, error in the language model will not override\nthis. The end-effector is also clamped within the predefined workspace\nbounds of x = [0.0, 1.1], y = [‚Äì0.3, 0.3] and z = [0, 1.0]. This is checked in\nfuture time steps by a publisher at a frequency of 10 Hz.",
      "grounding": [
        {
          "box": {
            "l": 0.06163206696510315,
            "t": 0.05690032243728638,
            "r": 0.5005087852478027,
            "b": 0.25097009539604187
          },
          "page": 7
        }
      ],
      "chunk_type": "text",
      "chunk_id": "2ef5c1c4-2995-44b0-8a4f-274500d29731"
    },
    {
      "text": "**Data availability**  \nThe dataset used in this work is available in an open-source GitHub repository at https://github.com/ruaidhmon/ELLMER.",
      "grounding": [
        {
          "box": {
            "l": 0.06171038746833801,
            "t": 0.25958526134490967,
            "r": 0.4981429874897003,
            "b": 0.3063412308692932
          },
          "page": 7
        }
      ],
      "chunk_type": "text",
      "chunk_id": "be3cae3f-2db8-46ef-baa6-6bae8d57cadb"
    },
    {
      "text": "**Code availability**  \nThe code supporting this study is available via GitHub at https://github.com/ruairidhmon/ELLMER and has been archived in Zenodo at https://doi.org/10.5281/zenodo.14483539 (ref. 70).",
      "grounding": [
        {
          "box": {
            "l": 0.06198471784591675,
            "t": 0.31358301639556885,
            "r": 0.4986978769302368,
            "b": 0.3746225833892822
          },
          "page": 7
        }
      ],
      "chunk_type": "text",
      "chunk_id": "8dd9f139-b678-4f61-ab69-f23a2fb0e1d5"
    },
    {
      "text": "**References**\n1. Intelligence research should not be held back by its past. *Nature* **545**, 385‚Äì386 (2017).\n2. Friston, K. Embodied inference and spatial cognition. *Cogn. Process.* **13**, 497‚Äì514 (2012).\n3. Wilson, M. Six views of embodied cognition. *Psychon. Bull. Rev.* **9**, 625‚Äì636 (2002).\n4. Clark, A. An embodied cognitive science. *Trends Cogn. Sci.* **3**, 345‚Äì351 (1999).\n5. Stella, F., Della Santina, C. & Hughes, J. How can LLMs transform the robotic design process? *Nat. Mach. Intell.* **5**, 561‚Äì564 (2023).\n6. Miriyev, A. & Kovac, M. Skills for physical artificial intelligence. *Nat. Mach. Intell.* **2**, 658‚Äì660 (2020).\n7. Cui, J. & Trinkle, J. Toward next-generation learned robot manipulation. *Sci. Robot.* **6**, eabd9461 (2021).\n8. Arents, J. & Greitans, M. Smart industrial robot control trends, challenges and opportunities within manufacturing. *Appl. Sci.* **12**, 937 (2022).\n9. Billard, A. & Kragic, D. Trends and challenges in robot manipulation. *Science* **364**, eaat8414 (2019).\n10. Yang, G.-Z. et al. The grand challenges of *Science Robotics*. *Sci. Robot.* **3**, eaar7650 (2018).\n11. Buchanan, R., Rofer, A., Moura, J., Valada, A. & Vijayakumar, S. Online estimation of articulated objects with factor graphs using vision and proprioceptive sensing. In *2024 IEEE International Conference on Robotics and Automation (ICRA)* 16111‚Äì16117 (IEEE, 2024).\n12. Nikolaidis, S., Ramakrishnan, R., Gu, K. & Shah, J. Efficient model learning from joint-action demonstrations for human-robot collaborative tasks. In *2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)* 189‚Äì196 (IEEE, 2015).\n13. Saveriano, M., Abu-Dakka, F. J., Kramberger, A. & Peternel, L. Dynamic movement primitives in robotics: a tutorial survey. *Int. J. Robot. Res.* **42**, 1133‚Äì1184 (2023).\n14. Kober, J. et al. Movement templates for learning of hitting and batting. In 2010 *IEEE International Conference on Robotics and Automation* 853‚Äì858 (IEEE, 2010).\n15. Huang, W. et al. VoxPoser: composable 3D value maps for robotic manipulation with language models. In *Proc. 7th Conference on Robot Learning* 540‚Äì562 (PMLR, 2023).",
      "grounding": [
        {
          "box": {
            "l": 0.05944618582725525,
            "t": 0.38405120372772217,
            "r": 0.49818095564842224,
            "b": 0.9437143802642822
          },
          "page": 7
        }
      ],
      "chunk_type": "text",
      "chunk_id": "b359775e-1706-48e7-84b6-8b147989ccfd"
    },
    {
      "text": "16. Zhang, D. et al. Explainable hierarchical imitation learning for\nrobotic drink pouring. In IEEE Transactions on Automation Science\nand Engineering 3871‚Äì3887 (2022).\n17. Hussein, A., Gaber, M. M., Elyan, E. & Jayne, C. Imitation learning:\na survey of learning methods. ACM Comput. Surv. 50, 21:1‚Äì21:35\n(2017).\n18. Di Palo, N. & Johns, E. DINObot: robot manipulation via retrieval\nand alignment with vision foundation models. In International\nConference on Robotics and Automation (ICRA) 2798‚Äì805\n(IEEE, 2024).\n19. Shridhar, M., Manuelli, L. & Fox, D. CLIPort: what and where\npathways for robotic manipulation. In Proc. 5th Conference on\nRobot Learning 894‚Äì906 (PMLR, 2022).\n20. Shridhar, M., Manuelli, L. & Fox, D. Perceiver-Actor: a multi-task\ntransformer for robotic manipulation. In Proc. 6th Conference on\nRobot Learning 785‚Äì799 (PMLR, 2023).\n21. Mees, O., Hermann, L. & Burgard, W. What matters in language\nconditioned robotic imitation learning over unstructured data.\nIEEE Robot. Autom. Lett. 7, 11205‚Äì11212 (2022).\n22. Mees, O., Borja-Diaz, J. & Burgard, W. Grounding language\nwith visual affordances over unstructured data. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA)\n11576‚Äì11582 (IEEE, 2023).\n23. Shao, L., Migimatsu, T., Zhang, Q., Yang, K. & Bohg, J.\nConcept2Robot: learning manipulation concepts from\ninstructions and human demonstrations. Int. J. Robot. Res. 40,\n1419‚Äì1434 (2021).\n24. Ichter, B. et al. Do as I can, not as I say: grounding language in\nrobotic affordances. In Proc. 6th Conference on Robot Learning\n287‚Äì318 (PMLR, 2023).\n25. Driess, D. et al. PaLM-E: an embodied multimodal language\nmodel. In Proc. 40th International Conference on Machine\nLearning 8469‚Äì8488 (PMLR, 2023).\n26. Peng, A. et al. Preference-conditioned language-guided\nabstraction. In Proc. 2024 ACM/IEEE International Conference\non Human-Robot Interaction, HRI ‚Äô24 572‚Äì581 (Association for\nComputing Machinery, 2024).\n27. Huang, W., Abbeel, P., Pathak, D. & Mordatch, I. Language models\nas zero-shot planners: extracting actionable knowledge for\nembodied agents. In Proc. 39th International Conference on\nMachine Learning 9118‚Äì9147 (PMLR, 2022).\n28. Huang, J. & Chang, K. C.-C. Towards reasoning in large language\nmodels: a survey. In Findings of the Association for Computational\nLinguistics: ACL 2023 1049‚Äì1065 (Association for Computational\nLinguistics, 2023).\n29. Zitkovich, B. et al. RT-2: vision-language-action models transfer\nweb knowledge to robotic control. In Proc. 7th Conference on\nRobot Learning 2165‚Äì2183 (PMLR, 2023).\n30. Ma, X., Patidar, S., Haughton, I. & James, S. Hierarchical diffusion\npolicy for semantics-aware multi-task robotic manipulation.\nIn Proc. IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) 18081‚Äì18090 (IEEE, 2024).\n31. Zhang, C., Chen, J., Li, J., Peng, Y. & Mao, Z. Large language\nmodels for human-robot interaction: a review. Biomimetic Intell.\nRobot. 3, 100131 (2023).\n32. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks. In Advances in Neural Information Processing\nSystems 9459‚Äì9474 (Curran Associates, 2020).\n33. Raiaan, M. et al. A review on large language models:\narchitectures, applications, taxonomies, open issues and\nchallenges. IEEE Access 12, 26839‚Äì26874 (2024).\n34. Rozo, L., Jimenez, P. & Torras, C. Force-based robot learning of\npouring skills using parametric hidden Markov models. In 9th\nInternational Workshop on Robot Motion and Control 227‚Äì232\n(IEEE, 2013).",
      "grounding": [
        {
          "box": {
            "l": 0.5103038549423218,
            "t": 0.05843096971511841,
            "r": 0.9444098472595215,
            "b": 0.9421233534812927
          },
          "page": 7
        }
      ],
      "chunk_type": "text",
      "chunk_id": "f5520add-2b55-4515-9741-f466e3a09b98"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.06270396709442139,
            "t": 0.9591014981269836,
            "r": 0.4507126808166504,
            "b": 0.9772817492485046
          },
          "page": 7
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "dc78ffb7-bbbe-4a07-b153-93a8463cf744"
    },
    {
      "text": "599",
      "grounding": [
        {
          "box": {
            "l": 0.9149916172027588,
            "t": 0.9622668027877808,
            "r": 0.9498438835144043,
            "b": 0.9773827791213989
          },
          "page": 7
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "7bf954be-2db6-417d-ab4f-30f13ce280d6"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.060975540429353714,
            "t": 0.02530743181705475,
            "r": 0.1277187466621399,
            "b": 0.04542475938796997
          },
          "page": 8
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "ed38b625-725f-427d-b2f1-3788ce367051"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6485903263092041,
            "t": 0.025519005954265594,
            "r": 0.9485340118408203,
            "b": 0.04536166042089462
          },
          "page": 8
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "87d9440c-22f0-421c-8d74-68e732a60484"
    },
    {
      "text": "35. Huang, Y., Wilches, J. & Sun, Y. Robot gaining accurate pouring skills through self-supervised learning and generalization. Robot. Auton. Syst. **136**, 103692 (2021).\n36. Mon-Williams, R., Stouraitis, T. & Vijayakumar, S. A behavioural transformer for effective collaboration between a robot and a non-stationary human. In 2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN) 1150‚Äì1157 (IEEE, 2023).\n37. Belkhale, S., Cui, Y. & Sadigh, D. Data quality in imitation learning. In Advances in Neural Information Processing Systems (NeurIPS) 80375‚Äì80395 (Curran Associates, 2024).\n38. Khazatsky, A. et al. DROID: a large-scale in-the-wild robot manipulation dataset. Robotics: Science and Systems; https://www.roboticsproceedings.org/rss20/p120.pdf (2024).\n39. Acosta, B., Yang, W. & Posa, M. Validating robotics simulators on real-world impacts. IEEE Robot. Autom. Lett. **7**, 6471‚Äì6478 (2022).\n40. Alomar, A. et al. CausalSim: a causal framework for unbiased trace-driven simulation. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23) 1115‚Äì1147 (USENIX Association, 2023).\n41. Choi, H. et al. On the use of simulation in robotics: opportunities, challenges, and suggestions for moving forward. Proc. Natl. Acad. Sci. USA **118**, e190785611 (2021).\n42. Del Aguila Ferrandis, J., Moura, J. & Vijayakumar, S. Nonprehensile planar manipulation through reinforcement learning with multimodal categorical exploration. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 5606‚Äì5613 (IEEE, 2023).\n43. Kirk, R., Zhang, A., Grefenstette, E. & Rockt√§schel, T. A survey of zero-shot generalisation in deep reinforcement learning. J. Artific. Intell. Res. **76**, 201‚Äì264 (2023).\n44. Dai, T. et al. Analysing deep reinforcement learning agents trained with domain randomisation. Neurocomputing **493**, 143‚Äì165 (2022).\n45. Chang, J., Uehara, M., Sreenivas, D., Kidambi, R. & Sun, W. Mitigating covariate shift in imitation learning via offline data with partial coverage. In Advances in Neural Information Processing Systems 965‚Äì979 (Curran Associates, 2021).\n46. Huang, W. et al. Inner monologue: embodied reasoning through planning with language models. In Proc. 6th Conference on Robot Learning 1769‚Äì1782 (PMLR, 2023).\n47. Nair, S., Rajeswaran, A., Kumar, V., Finn, C. & Gupta, A. R3M: a universal visual representation for robot manipulation. In Proc. 6th Conference on Robot Learning Vol. 205, 892‚Äì909 (PMLR, 2022).\n48. Singh, I. et al. ProgPrompt: generating situated robot task plans using large language models. In Proc. IEEE/CVF International Conference on Robotics and Automation (ICRA) 11523‚Äì11530 (IEEE, 2023).\n49. Song, C. H. et al. LLM-Planner: few-shot grounded planning for embodied agents with large language models. In Proc. IEEE/CVF International Conference on Computer Vision (ICCV) 2998‚Äì3009 (IEEE/CVF, 2023).\n50. Vemprala, S. H., Bonatti, R., Bucker, A. & Kapoor, A. ChatGPT for robotics: design principles and model abilities. IEEE Access 12, 55682‚Äì55696 (2024).\n51. Ding, Y., Zhang, X., Paxton, C. & Zhang, S. Task and motion planning with large language models for object rearrangement. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2086‚Äì2092 (IEEE, 2023).\n52. Kwon, M. et al. Toward grounded commonsense reasoning. In Proc. International Conference on Robotics and Automation (ICRA) 5463‚Äì5470 (IEEE, 2024).\n53. Hong, J., Levine, S. & Dragan, A. Learning to influence human behavior with offline reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS) 36094‚Äì36105 (Curran Associates, 2023).",
      "grounding": [
        {
          "box": {
            "l": 0.05861295759677887,
            "t": 0.053366780281066895,
            "r": 0.4982988238334656,
            "b": 0.9459397792816162
          },
          "page": 8
        }
      ],
      "chunk_type": "text",
      "chunk_id": "51ae8d76-3ef8-455d-8ebf-22e2085d64e9"
    },
    {
      "text": "54. OpenAI. GPT-4 technical report. Preprint at http://arxiv.org/ abs/2303.08774 (2024).\n55. OpenAI. Custom models program: fine-tuning GPT-4 for specific domains (2023); https://platform.openai.com/docs/guides/ fine-tuning/\n56. Pietsch, M. et al. Haystack: the end-to-end nlp framework for pragmatic builders. GitHub https://github.com/deepset-ai/ haystack (2019).\n57. Weaviate. Verba: the golden RAGretriever. GitHub https://github. com/weaviate/Verba (2023).\n58. Kirillov, A. et al. Segment anything. In Proc. IEEE/CVF International Conference on Computer Vision (ICCV) 4015‚Äì4026 (IEEE, 2023).\n59. Ramesh, A. et al. Zero-shot text-to-image generation. In Proc. 38th International Conference on Machine Learning 8281‚Äì8831 (PMLR, 2021).\n60. Zeng, A. et al. Socratic models: composing zero-shot multimodal reasoning with language. In Proc. International Conference on Learning Representations (ICLR, 2023).\n61. Cui, Y. et al. No, to the right: online language corrections for robotic manipulation via shared autonomy. In Proc. 2023 ACM/ IEEE International Conference on Human-Robot Interaction, HRI ‚Äô23 93‚Äì101 (Association for Computing Machinery, 2023).\n62. Bengio, Y. et al. Managing extreme AI risks amid rapid progress. Science 384, 842‚Äì845 (2024).\n63. Li, G., Jampani, V., Sun, D. & Sevilla-Lara, L. Locate: localize and transfer object parts for weakly supervised affordance grounding. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 10922‚Äì10931 (IEEE, 2023).\n64. Li, G., Sun, D., Sevilla-Lara, L. & Jampani, V. One-shot open set affordance learning with foundation models. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 3086‚Äì3096 (IEEE, 2024).\n65. Liang, J. et al. Code as policies: language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA) 9493‚Äì9500 (IEEE, 2023).\n66. Hong, S. & Kim, H. An integrated GPU power and performance model. In Proc. 37th Annual International Symposium on Computer Architecture 280‚Äì289 (Association for Computing Machinery, 2010).\n67. Kinova Robotics. Kinova Gen3 Ultra-Lightweight Robotic Arm User Guide (2023); https://assets.iqr-robornt.com/wp-content/ uploads/2023/08/20230814163651088831.pdf\n68. US Environmental Protection Agency. GHG emission factors hub (2024); https://www.epa.gov/climateleadership/ ghg-emission-factors-hub\n69. Liu, S. et al. Grounding DINO: marrying DINO with grounded pre-training for open-set object detection. In 2024 European Conference on Computer Vision (eds Iandola, S. et al.) Vol. 15105 (Springer, 2023).\n70. ruadhrin/noa.foundation/ELLMER: v1.0.0: Initial Release. Zenodo https://doi.org/10.5281/zenodo.14483539 (2024).",
      "grounding": [
        {
          "box": {
            "l": 0.508133053779602,
            "t": 0.05331212282180786,
            "r": 0.948424220085144,
            "b": 0.7676143646240234
          },
          "page": 8
        }
      ],
      "chunk_type": "text",
      "chunk_id": "0b1f6d83-bcf4-45f4-9dd7-58655d768e16"
    },
    {
      "text": "**Acknowledgements**  \nThis work was supported by the EPSRC CDT in RAS (EP/L016834/1). We thank S. Vijayakumar for his support and for providing access to resources; L. Martins and the Edinburgh Workshop for their assistance with hardware; and J. Wang, T. Stouraitis, J. Ferrandis and many others for their invaluable support and expertise.",
      "grounding": [
        {
          "box": {
            "l": 0.5096849799156189,
            "t": 0.7752266526222229,
            "r": 0.9428208470344543,
            "b": 0.863368570804596
          },
          "page": 8
        }
      ],
      "chunk_type": "text",
      "chunk_id": "586953c6-9630-4876-854c-debc6757b09e"
    },
    {
      "text": "Author contributions  \nConceptualization: R.M.-W., G.L. and R.L. Methodology: R.M.-W., G.L., R.L., W.D. and C.G.L. Software: R.M.-W., G.L., R.L. and W.D. Formal analysis: R.M.-W. Investigation: R.M.-W. and W.D. Visualization: R.M.-W. Validation: R.M.-W. and W.D. Writing‚Äîoriginal draft:",
      "grounding": [
        {
          "box": {
            "l": 0.5096012949943542,
            "t": 0.8714070916175842,
            "r": 0.9453313946723938,
            "b": 0.945025622844696
          },
          "page": 8
        }
      ],
      "chunk_type": "text",
      "chunk_id": "300d7da4-aef5-4f1d-8e57-f65a4177b2e4"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.062117382884025574,
            "t": 0.9592235088348389,
            "r": 0.4519261121749878,
            "b": 0.9771651029586792
          },
          "page": 8
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "6a4e850a-0c86-4878-9118-32362eff1746"
    },
    {
      "text": "600",
      "grounding": [
        {
          "box": {
            "l": 0.9120545387268066,
            "t": 0.9624727964401245,
            "r": 0.9482293128967285,
            "b": 0.9769606590270996
          },
          "page": 8
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "538a3b37-da48-43a1-9a82-7ba0de4a4743"
    },
    {
      "text": "Article",
      "grounding": [
        {
          "box": {
            "l": 0.0610702745616436,
            "t": 0.025180984288454056,
            "r": 0.12802571058273315,
            "b": 0.045342858880758286
          },
          "page": 9
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "f295f87e-0849-4b89-9605-9b1ca4bd27f2"
    },
    {
      "text": "https://doi.org/10.1038/s42256-025-01005-x",
      "grounding": [
        {
          "box": {
            "l": 0.6481989622116089,
            "t": 0.0258815735578537,
            "r": 0.948471188545227,
            "b": 0.045386724174022675
          },
          "page": 9
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "3f0da87b-5802-49ef-9236-387a64dbb828"
    },
    {
      "text": "R.M.-W. Writing‚Äîreview and editing: R.M.-W., G.L., R.L. and C.G.L.\nSupervision: C.G.L.",
      "grounding": [
        {
          "box": {
            "l": 0.061494261026382446,
            "t": 0.05581267923116684,
            "r": 0.4736512005329132,
            "b": 0.08834255486726761
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "4f79f0d3-6694-42f8-a76a-4eaa936f27ed"
    },
    {
      "text": "**Competing interests**  \nThe authors declare no competing interests.",
      "grounding": [
        {
          "box": {
            "l": 0.06171722710132599,
            "t": 0.09631866216659546,
            "r": 0.3411320447921753,
            "b": 0.12981204688549042
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "c818f647-cba5-4569-83b1-fe770e7defab"
    },
    {
      "text": "## Additional information\n**Supplementary information** The online version contains supplementary material available at https://doi.org/10.1038/s42256-025-01005-x.",
      "grounding": [
        {
          "box": {
            "l": 0.06140141189098358,
            "t": 0.13777171075344086,
            "r": 0.4978370666503906,
            "b": 0.18428398668766022
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "c2c6ad6e-2ae3-41d3-adb5-cb22796da5fe"
    },
    {
      "text": "**Correspondence and requests for materials** should be addressed to Ruaridh Mon-Williams or Gen Li.",
      "grounding": [
        {
          "box": {
            "l": 0.061949267983436584,
            "t": 0.1938467174768448,
            "r": 0.4877163767814636,
            "b": 0.22452528774738312
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "9b75625d-2507-4e76-8d63-035c1d15dfd3"
    },
    {
      "text": "**Peer review information** *Nature Machine Intelligence* thanks Matteo Saveriano and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.",
      "grounding": [
        {
          "box": {
            "l": 0.06228649616241455,
            "t": 0.23504352569580078,
            "r": 0.49451011419296265,
            "b": 0.2788097858428955
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "1f7686cb-c7a4-4104-9f97-e0239f84e60f"
    },
    {
      "text": "**Reprints and permissions information** is available at  \nwww.nature.com/reprints.",
      "grounding": [
        {
          "box": {
            "l": 0.06217339634895325,
            "t": 0.2901230752468109,
            "r": 0.38884565234184265,
            "b": 0.3188602030277252
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "843ed970-d88d-4cb6-84f6-ebf317ffc3ad"
    },
    {
      "text": "**Publisher‚Äôs note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
      "grounding": [
        {
          "box": {
            "l": 0.510432243347168,
            "t": 0.056765079498291016,
            "r": 0.8889555931091309,
            "b": 0.10218563675880432
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "b3799e2d-fab1-48ec-b248-b33d72eadd80"
    },
    {
      "text": "Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.",
      "grounding": [
        {
          "box": {
            "l": 0.5094717144966125,
            "t": 0.11282552033662796,
            "r": 0.9418105483055115,
            "b": 0.2922245264053345
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "0d4b48d5-babd-4bcb-834e-452c81c3f208"
    },
    {
      "text": "¬© The Author(s) 2025",
      "grounding": [
        {
          "box": {
            "l": 0.5103609561920166,
            "t": 0.3026313781738281,
            "r": 0.6487579345703125,
            "b": 0.32055526971817017
          },
          "page": 9
        }
      ],
      "chunk_type": "text",
      "chunk_id": "7e3ce6ff-bb67-4116-a8ef-28b06a52dc22"
    },
    {
      "text": "Nature Machine Intelligence | Volume 7 | April 2025 | 592‚Äì601",
      "grounding": [
        {
          "box": {
            "l": 0.062080323696136475,
            "t": 0.959646999835968,
            "r": 0.4511711001396179,
            "b": 0.9776087403297424
          },
          "page": 9
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "8929aeeb-7c55-4537-a7ec-05e8be64d31f"
    },
    {
      "text": "601",
      "grounding": [
        {
          "box": {
            "l": 0.9169490337371826,
            "t": 0.9625693559646606,
            "r": 0.9504109621047974,
            "b": 0.9774439334869385
          },
          "page": 9
        }
      ],
      "chunk_type": "marginalia",
      "chunk_id": "d15b5638-6c58-4b3c-a6df-521396939a56"
    }
  ],
  "extracted_schema": null,
  "extraction_metadata": null
}